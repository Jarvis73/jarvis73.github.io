---
layout: post
title: "t-SNE 高维数据可视化"
date: 2020-08-24 14:15:00 +0800
categories: 可视化
mathjax: true
figure: ./images/2020-08/tSNE-2.png
author: Jarvis
meta: Post
---

* content
{:toc}




>   本文内容主要翻译自 Visualizating Data using t-SNE[^1].



## 1. Introduction

高维数据可视化是许多领域的都要涉及到的一个重要问题. **降维 (dimensionality reduction)** 是把高维数据转化为二维或三维数据从而可以通过散点图展示的方法. **降维的目标是尽可能多的在低维空间保留高维数据的关键结构.** 传统降维方法如 PCA[^2], MDS[^3] 都是线性方法, 在降维过程中主要<u>保证了不相似的点尽量远离</u>的结构特征. 但是当高维数据处于低维非线性流形上时, <u>保证相似的点尽量接近</u>更为重要, 而这在线性映射中通过难以做到. 

目前已经有大量非线性降维技术, 可以参考 Lee 和 Verleysen 在 2007 年的综述论文[^4]. 这里列举七种方法:

*   Sammon mapping[^5] 
*   Curvilinear components analysis, CCA[^6] 
*   Stochastic Neighbor Embedding, SNE[^7] 
*   Isomap[^8] 
*   Maximum Variance Unfolding, MVU[^9] 
*   Locally Linear Embedding, LLE[^10] 
*   Laplacian Eigenmaps[^11] 

本文提出 t-SNE 用于可视化高维数据, 它可以保留高维数据的局部特征, 同时也能揭示数据的整体结构. 
$$\newcommand{\pjci}{p_{j\vert i}} \newcommand{\qjci}{q_{j\vert i}} \newcommand{\pij}{p_{ij}} \newcommand{\qij}{q_{ij}} \newcommand{\pji}{p_{ji}} \newcommand{\qji}{q_{ji}}$$ 

## 2. SNE

**随机近邻嵌入 (stochastic neighbor embedding, SNE)** 把高维数据点之间的<u>欧氏距离</u>转化为<u>表示相似度的条件概率</u>. 数据点 $$x_j$$ 与  $$x_i$$ 的相似度定义为条件概率 $$p_{j\vert i}$$ , 它表示数据点 $$x_i$$ 选择邻近点 $$x_j$$ , 且该条件概率正比于在点 $$x_i$$ 做高斯中心化后的概率密度, 如下式所示:

$$
\pjci=\frac{\exp\left(-\Vert x_i-x_j\Vert^2/2\sigma_i^2\right)}{\sum_{k\neq i}\exp\left(-\Vert x_i-x_k\Vert^2/2\sigma_i^2\right)}
$$

其中 $$\sigma_i$$ 是中心在 $$x_i$$ 高斯分布的标准差. 如果两个点离得很近, $$p_{j\vert i}$$ 就会比较大; 反之就会接近 0. 我们定义 $$p_{i\vert i}=0$$ . 对于高维点 $$x_i$$ 和 $$x_j$$ 对应的低维点 $$y_i$$ 和 $$y_j$$ , 我们可以计算一个类似地相似度, 记为 $$q_{j\vert i}$$ . 我们固定低维空间中用于计算 $$q_{j\vert i}$$ 的高斯分布的标准差为 $$1/\sqrt2$$ , 这样我们有:

$$
\qjci=\frac{\exp\left(-\Vert y_i-y_j\Vert^2\right)}{\sum_{k\neq i}\exp\left(-\Vert y_i-y_k\Vert^2\right)}
$$

按照假设, 如果 $$q_{j\vert i}$$ 可以正确建模高维数据点之间的相似度, 那么条件概率 $$p_{j\vert i}$$ 和 $$q_{j\vert i}$$ 应该是相等的. 因此 SNE 的目标就转化为减小这两个条件概率之间的误差. 一个自然的想法是利用 **KL 散度 (Kullback-Leibler divergence)** . 我们使用梯度下降法令 SNE 极小化所有数据点的 KL 散度, 损失函数定义为:

$$
C=\sum_i KL(P_i\Vert Q_i)=\sum_i\sum_j \pjci\log\frac{\pjci}{\qjci}
$$

其中 $$P_i$$ 表示给定数据点 $$x_i$$ 时关于其他所有数据点的条件概率分布, 而 $$Q_i$$ 表示给定降维后的数据点 $$y_i$$ 时关于其他所有数据点的条件概率分布. 由于 KL 散度是**非对称的**, 所以我们有如下的观察:

*   如果用<u>低维空间</u>中距离**远**的两个点 ( $$\qjci$$ 小) 来表示<u>高维空间</u>中距离**近**的两个点 ( $$\pjci$$ 大 ), 那么就会得到一个较**大**的损失
*   如果用<u>低维空间</u>中距离**近**的两个点 ( $$\qjci$$ 大) 来表示<u>高维空间</u>中距离**远**的两个点 ( $$\pjci$$ 小 ), 那么就会得到一个较**小**的损失

从这两点观察可以总结出: SNE 损失函数目标是尽可能保持数据的局部结构 (即距离近的点在低维空间中映射的不能太远). 

另外要考虑的是参数 $$\sigma_i$$ 的选取. 由于数据点密度的不同, 我们不应该用单个 $$\sigma_i$$ 的值来建模所有的数据点. 在数据稠密的区域, 用小的 $$\sigma_i$$ 显然要比大的值更准确. 任意 $$\sigma_i$$ 的值都对应一个概率分布 $$P_i$$ , 它可以关联到熵, 随着 $$\sigma_i$$ 的增大而增大 (表示数据点的分散程度). SNE 通过给定一个固定的**困惑度 (perplexity)** 后进行二分搜索来寻找 $$\sigma_i$$ , 从而确定 $$P_i$$. 这里定义困惑度 为:

$$
Perp(P_i)=2^{H(P_i)}
$$

其中 $$H(P_i)$$ 为香农熵:

$$
H(P_i)=-\sum_j \pjci\log_2\pjci
$$

困惑度可以解释为有效邻近点个数的光滑化的测度.  SNE 对困惑度的调整有一定鲁棒性, 典型值为 $$5\sim50$$ . 

公式 (3) 的极小化可以通过梯度下降法来计算. 其梯度为:

$$
\frac{\partial C}{\partial y_i} = 2\sum_j (\pjci-\qjci+p_{i\vert j}-q_{i\vert j})(y_i-y_j)
$$

为了加速收敛并避免陷入局部极小值点, 在梯度下降的过程中要使用动量 (即加上先前梯度的指数衰减项):

$$
\mathcal{Y}^{(t)}=\mathcal{Y}^{(t-1)}+\eta\frac{\partial C}{\partial \mathcal{Y}}+\alpha(t)\left(\mathcal{Y}^{(t-1)}-\mathcal{Y}^{(t-2)}\right)
$$


## 3. t-SNE

尽管 SNE 已经可以给出较好的数据可视化, 但它仍然受限于优化问题的困难和"拥挤问题". 本节我们提出 t-SNE 来解决该问题. 与 SNE 相比, t-SNE 的损失函数有两点不同:

1.  它使用了一种**对称的**损失函数 (梯度更简洁)
2.  它使用了 t-分布代替高斯分布在低维空间表示两点间的相似度 (采用长尾分布来减轻"拥挤问题")

### 3.1 对称 SNE

除了直接优化条件概率 $$p_{j\vert i}$$ 和 $$q_{j\vert i}$$ 之间的 KL 散度, 还可以优化两个联合分布 (高维空间的 P 分布和低维空间的 Q 分布) 的 KL 散度:

$$
C=KL(P\Vert Q)=\sum_i\sum_j \pij\log\frac{\pij}{\qij}
$$

与之前类似, 我们令 $$p_{ii}=0,q_{ii}=0$$ . 我们把这种 SNE 称为对称 SNE, 因为满足  $$\pij=\pji,\qij=\qji,\forall i,j$$. 在对称 SNE 中, 低维空间两点的相似度为:

$$
\qij=\frac{\exp\left(-\Vert y_i-y_j\Vert^2\right)}{\sum_{k\neq l}\exp\left(-\Vert y_k-y_l\Vert^2\right)}
$$

类似地高维空间两点相似度为:

$$
\pij=\frac{\exp\left(-\Vert x_i-x_j\Vert^2/2\sigma^2\right)}{\sum_{k\neq l}\exp\left(-\Vert x_k-x_l\Vert^2/2\sigma^2\right)}\nonumber
$$

注意到如果有离群点 $$x_i$$, 那么 $$\pij$$ 的值就会对所有的 $$x_j$$ 都非常小 (因为 $$\Vert y_i-y_j\Vert^2$$ 非常大) , 这样对损失函数的影响就很小, 导致低维空间中无法很好的捕获离群点和其他点之间的关系. 为了避免这个问题, 我们把联合概率 $$\pij$$ 定义为:

$$
\pij=\frac{\pjci+p_{i\vert j}}{2n}
$$

这保证了对于所有的 $$x_i$$ , 都有 $$\sum_j\pij>\frac1{2n}$$ , 从而每一项都会对损失函数有可观的贡献. 在低维空间中对称 SNE 的计算直接使用 (9) 式. 对称 SNE 的一个主要的优点是梯度简洁容易计算:

$$
\frac{\partial C}{\partial y_i}=4\sum_j(\pij-\qij)(y_i-y_j)
$$

梯度的计算方式见附录.

### 3.2 拥挤问题

先考虑一种简单的情况: 一族点分布在一个二维流形上, 其分布呈近似的直线型, 该流行嵌入到高维空间中. 那么我们可以比较容易地对点之间的距离在二维空间中建模. 

再考虑一种复杂的情况: 流行有10个本征维度并嵌入到一个更高维的空间中, 此时对点的距离进行建模就会遇到问题:

*   比如, 在10维空间, 存在11个点构成的正多面体, 任意两点之间的距离都相等. 这种情况下不可能在二维空间中对其建模. 
*   比如, 一个点为高维球心, 其余点均匀的分布在高维球的表面, 这是个10维流行, 那么对球心到其他点在低维空间的建模也会遇到困难, 称为**拥挤问题**: 
    *   在高维空间中距离近的点被映射到低维空间的距离仍然可以比较近
    *   在高维空间中距离远的点被映射到低维空间后无法使得距离足够远, 即低维空间无法分散地容纳高维空间中远距离的点, 这是由降维造成的, 从而这些远距离的点在高维空间可能是分散的, 但在低维空间就挤在了一起. 
*   SNE 方法并没有解决这个问题. 要解决该问题, 需要对低维空间用于建模的分布做一定的调整.

### 3.3 调整低维空间的分布

因为对称 SNE 实际上匹配的是高维空间和低维空间数据的联合分布而非距离, 因此我们可以通过以下的方法来解决拥挤问题:

*   在高维空间, 我们用高斯分布把距离转化为概率
*   在低维空间, 我们用长尾分布把距离转化为概率

这样高维空间里的中等距离映射到低维空间中有一个较远的距离, 这样可以避免不相似的点离得太近. 在 t-SNE 中, 我们使用**自由度为 1 的 t-分布** (也称为柯西分布) 作为低维空间中的长尾分布, 这样联合概率就变为:

$$
\qij=\frac{(1+\Vert y_i-y_j\Vert^2)^{-1}}{\sum_{k\neq l}(1+\Vert y_k-y_l\Vert^2)^{-1}}
$$

我们采用自由度为 1 的 t-分布是因为 $$(1+\Vert y_i-y_j\Vert^2)^{-1}$$ 在 $$\Vert y_i-y_j\Vert$$ 距离较大时接近平方倒数. 这使得联合分布的低维表示对于低维尺度的缩放有着一定的不变性. 我们选择 t-分布的原因是它是一种无限的高斯混合分布, 但计算概率密度时却不再需要计算指数, 可以降低计算复杂度. 

高维分布 $$P$$ 和低维 t-分布 $$Q$$ 的 KL 散度的梯度可以根据附录类似地计算:

$$
\frac{\partial C}{\partial y_i}=4\sum_j(\pij-\qij)(y_i-y_j)(1+\Vert y_i-y_j\Vert^2)^{-1}
$$

下图展示了 $$\Vert x_i-x_j\Vert$$ 和 $$\Vert y_i-y_j\Vert$$ 和梯度的关系, 三幅图分别为对称 SNE, UNI-SNE 和 t-SNE 的结果.

{% include image.html class="polaroid" url="2020-08/tSNE-1.png" title="三种类型 SNE 的梯度作为高维点距和低维点距函数的热力图" %}

其中正梯度表示两点之间存在引力, 负梯度表示斥力. 从上图可以发现 t-SNE 有以下优点:

*   t-SNE 梯度对于低维中邻近的不相似点有较大的斥力, 相比于 SNE 和 UNI-SNE 在分布上更为均匀合理
*   t-SNE 的斥力是有限的, 不像 UNI-SNE 那样随着距离无限增大

### 3.4 算法

t-SNE 的一种简单算法

---
**数据**: 数据集 $$\mathcal{X}=\{x_1,x_2,\dots,x_n\}$$ 

损失函数的参数: 困惑度 $$Perp$$ 

优化算法的参数: 迭代次数 $$T$$ , 学习率 $$\eta$$ , 动量 $$\alpha(t)$$ 

**结果**: 低维数据表示 $$\mathcal{Y}^{(T)}=\{y_1,y_2,\dots,y_n\}$$ 

**begin**

&nbsp;&nbsp;&nbsp;&nbsp;使用 $$Perp$$ 计算相似度 $$\pjci$$  

&nbsp;&nbsp;&nbsp;&nbsp;令 $$\pij=\frac{\pjci+p_{i\vert j}}{2}$$  

&nbsp;&nbsp;&nbsp;&nbsp;从 $$\mathcal{N}(0,10^{-4}I)$$ 中采样初始结果 $$\mathcal{Y}^{(0)}=\{y_1,y_2,\dots,y_n\}$$ 

&nbsp;&nbsp;&nbsp;&nbsp;**for** $$t=1$$ **to** $$T$$ **do**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计算低维相似度 $$\qij$$ 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计算梯度 $$\frac{\partial C}{\partial \mathcal{Y}}$$ 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更新 $$\mathcal{Y}^{(t)}=\mathcal{Y}^{(t-1)}+\eta\frac{\partial C}{\partial\mathcal{Y}}+\alpha(t)(\mathcal{Y}^{(t-1)}-\mathcal{Y}^{(t-2)})$$ 

&nbsp;&nbsp;&nbsp;&nbsp;**end**

**end**

---


## 4. 可视化

代码实现: [Github: Jarvis73/MachineLearning](https://github.com/Jarvis73/MachineLearning) 

代码中包含了根据 [blog](https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/) 及其 [Github: nlml/tsne_raw](https://github.com/nlml/tsne_raw) 的实现, 以及基于 [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) 的实现. 后者在执行速度和效果上都有很大幅度的提升.

*   自行实现的结果

{% include image.html class="polaroid" url="2020-08/tSNE-3.png" title="raw: mnist, lr=10, perplexity=20, n_pts=1000, n_iter=500" %}

*   sklearn 的实现

{% include image.html class="polaroid" url="2020-08/tSNE-2.png" title="sklearn: mnist, lr=10, perplexity=20, n_pts=1000, n_iter=500" %}



## 附录

### 公式 (11) 的梯度计算.

我们把要用到的表达式先列在前面:

*   高维空间数据点的相似度为 $$p_{ij}$$ 
*   低维空间数据点的相似度为 

$$
\qij=\frac{\exp\left(-\Vert y_i-y_j\Vert^2\right)}{\sum_{k\neq l}\exp\left(-\Vert y_k-y_l\Vert^2\right)}
$$

*   t-SNE 损失函数 (公式 (11)) 为

$$
\begin{align}
C=KL(P\Vert Q) &=\sum_i\sum_j\pij\log\frac{\pij}{\qij} \\
&=\sum_i\sum_j\pij\log \pij-\pij\log\qij
\end{align}
$$

为简化计算, 我们定义两个辅助变量

$$
\begin{align}
d_{ij} &=\Vert y_i-y_j\Vert^2 \\
Z &=\sum_{j\neq l}\exp(-d_{kl})
\end{align}
$$

其中 $$Z$$ 就是 $$q_{ij}$$ 的分母, 所以有 $$q_{ij}Z=\exp(-d_{ij})$$ . 

注意到当 $$y_i$$ 变化一个微元时, 只有距离 $$d_{ij}$$ 和 $$d_{ji}$$ 发生变化, $$\forall j$$ . 因此 $$C$$ 关于 $$y_i$$ 的梯度为:

$$
\begin{align}\nonumber
\frac{\partial C}{\partial y_i} &= \sum_j\left(\frac{\partial C}{\partial d_{ij}}\frac{\partial d_{ij}}{\partial y_i} + \frac{\partial C}{\partial d_{ji}}\frac{\partial d_{ji}}{\partial y_i}\right) \\ \nonumber
&= 2\sum_j\frac{\partial C}{\partial d_{ij}}\frac{\partial d_{ij}}{\partial y_i} \qquad\qquad \Longleftarrow \qquad\qquad d_{ij}=d_{ji} \\ \nonumber
&= 2\sum_j\frac{\partial C}{\partial d_{ij}}\frac{\partial(\Vert y_i-y_j\Vert^2)}{\partial y_i} \\
&= 4\sum_j\frac{\partial C}{\partial d_{ij}}(y_i-y_j)
\end{align}
$$

接下来计算 $$\frac{\partial C}{\partial d_{ij}}$$ . 由于损失函数第一项关于 $$d_{ij}$$ 为常数, 因此只计算第二项的导数:

$$
\begin{align}
\frac{\partial C}{\partial d_{ij}} \nonumber
&= -\sum_{k\neq l}p_{kl}\frac{\partial(\log q_{kl})}{\partial d_{ij}} \\ \nonumber
&= -\sum_{k\neq l}p_{kl}\frac{\partial(\log q_{kl}Z-\log Z)}{\partial d_{ij}} \\ \nonumber
&= -\sum_{k\neq l}p_{kl}\left(\frac1{q_{kl}Z}\frac{\partial(\exp(-d_{kl}))}{\partial d_{ij}}-\frac1Z\frac{\partial Z}{\partial d_{ij}}\right) \\ \nonumber
&= \frac{\pij}{\qij Z}\exp(-d_{ij})-\sum_{k\neq l}p_{kl}\frac{\exp(-d_{ij})}{Z} &&\Longleftarrow\quad \frac{\partial(\exp(-d_{kl}))}{\partial d_{ij}}\neq 0 \text{ iff } d_{kl}=d_{ij} \\ \nonumber
&= \pij - \sum_{k\neq l}p_{kl}\frac{\exp(-d_{ij})}{Z} &&\Longleftarrow\quad \qij Z=\exp(-d_{ij}) \\
&= \pij - \qij &&\Longleftarrow\quad \sum_{k\neq l}p_{kl}=1,\;\; \qij Z=\exp(-d_{ij})
\end{align}
$$

综合以上两式的推导可得

$$
\frac{\partial C}{\partial y_i}=4\sum_j(\pij-\qij)(y_i-y_j)
$$


## 参考文献

[^1]:
    **Visualizing Data using t-SNE**<br />
    Laurens van der Maaten, Geoffrey Hinton <br />
    [[html]](https://www.jmlr.org/papers/v9/vandermaaten08a.html), [[pdf]](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), Journal of machine learning research, 2008, 9(Nov): 2579-2605.

[^2]:
    **Analysis of a complex of statistical variables into principal components**<br />
    Hotelling H <br />
    [[html]](https://psycnet.apa.org/record/1934-00645-001), Journal of educational psychology, 1933, 24(6): 417.

[^3]:
    **Multidimensional scaling: I. Theory and method**<br />
    Torgerson W S. <br />
    [[pdf]](https://link.springer.com/content/pdf/10.1007/BF02288916.pdf), Psychometrika, 1952, 17(4): 401-419.

[^4]:
    **Nonlinear dimensionality reduction of data manifolds with essential loops**<br />
    Lee J A, Verleysen M. <br />
    [[pdf]](https://link.springer.com/content/pdf/10.1007/BF02288916.pdf), Neurocomputing, 2005, 67: 29-53.

[^5]:
    **A nonlinear mapping for data structure analysis**<br />
    Sammon J W. <br />
    [[html]](https://ieeexplore.ieee.org/abstract/document/1671271/), IEEE Transactions on computers, 1969, 100(5): 401-409.

[^6]:
    **Curvilinear component analysis: A self-organizing neural network for nonlinear mapping of data sets**<br />
    Demartines P, Hérault J. <br />
    [[html]](https://ieeexplore.ieee.org/abstract/document/554199/), IEEE Transactions on neural networks, 1997, 8(1): 148-154.

[^7]:
    **Stochastic neighbor embedding**<br />
    Hinton G E, Roweis S T. <br />
    [[pdf]](http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf), In NIPS 2003: 857-864.

[^8]:
    **A global geometric framework for nonlinear dimensionality reduction**<br />
    Tenenbaum J B, De Silva V, Langford J C. <br />
    [[pdf]](https://link.springer.com/content/pdf/10.1007/BF02288916.pdf), science, 2000, 290(5500): 2319-2323.

[^9]:
    **Learning a kernel matrix for nonlinear dimensionality reduction**<br />
    Weinberger K Q, Sha F, Saul L K. <br />
    [[html]](https://dl.acm.org/doi/abs/10.1145/1015330.1015345), In ICML. 2004: 106.

[^10]:
    **Nonlinear dimensionality reduction by locally linear embedding**<br />
    Roweis S T, Saul L K. <br />
    [[html]](https://science.sciencemag.org/content/290/5500/2323.abstract), science, 2000, 290(5500): 2323-2326.

[^11]:
    **Laplacian eigenmaps and spectral techniques for embedding and clustering**<br />
    Belkin M, Niyogi P. <br />
    [[pdf]](http://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering.pdf), In NIPS. 2002: 585-591.
