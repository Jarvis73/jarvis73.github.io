---
layout: post
title: "视频目标识别: 光流网络(FlowNet)"
data: 2018-12-03 20:46:00
categories: 深度学习
mathjax: true
figure: /images/2018-12/FlowNet-4.jpg
author: Jarvis
meta: Post
---

* content
{:toc}

> **光流(Optical flow or optic flow)**是关于视域中的物体运动检测中的概念。用来描述相对于观察者的运动所造成的观测目标、表面或边缘的运动。光流法在样型识别、计算机视觉以及其他影像处理领域中非常有用，可用于运动检测、物件切割、碰撞时间与物体膨胀的计算、运动补偿编码，或者通过物体表面与边缘进行立体的测量等等。 



<div class="polaroid">
    <img class="cool-img" src="/images/2018-12/FlowNet-4.jpg" FlowNet/>
    <div class="container">
        <a href="https://en.wikipedia.org/wiki/Optical_flow">Opical Flow</a>
    </div>
</div>

## FlowNet

[原文链接](https://arxiv.org/pdf/1504.06852)

**想法**: 探索CNN在学习不同尺度上特征的能力, 以及通过这些特征寻找图像之间的关联.

### 相关工作

总结一下就是: (1)以前计算光流的方法都是从[Horn 和 Schunck的变分方法](https://dspace.mit.edu/bitstream/handle/1721.1/6337/%EE%80%80AIM%EE%80%81-572.pdf?sequence=2)衍生而来的(2)已有的用到CNN的方法都是基于小块(pased based)的. 所以本文是用完整图像计算光流的**头一篇**.

涉及到逐像素预测的CNN的应用包括(1)语义分割(2)深度预测(3)关键点检测(4)边缘检测. 当然了, 本文的光流预测也是逐像素的.

### 网络结构

<div class="polaroid">
    <img class="cool-img" src="/images/2018-12/FlowNet-1.jpg" FlowNet/>
    <div class="container">
        <p>FlowNetSimple 结构示意图</p>
    </div>
</div>

**FlowNetSimple**: 简单地把两个图片作为两个通道输入网络.

<div class="polaroid">
    <img class="cool-img" src="/images/2018-12/FlowNet-2.jpg" FlowNet/>
    <div class="container">
        <p>FlowNetCorr 结构示意图</p>
    </div>
</div>

**FlowNetCorr**: 两个图片分别在两个自网络学习, 中间通过`correlation layer`融合在一起. `correlation layer`是两组特征图之间的卷积, 即大小为`[h, w, c]`的图像(记为A)中的$k\times k$的patch和另一个大小为`[h, w, c]`图像(记为B)中的patch之间的卷积. 由于计算量太大($h^2\times w^2$次卷积), 所以本文假设位移大小是有限的(最大位移大小为$d$), 这样对于图A中的每一个anchor上的patch, 不必遍历图B的所有anchor, 仅需要遍历$D\times D$(其中$D=2d+1$)个anchor上的patch即可, 因此计算量降低到$h\times w\times D^2$. 此外我们给出卷积的公式:

$$
c(\mathbf{x}_1, \mathbf{x}_2) = \sum_{\mathbf{o}\in[-k, k]\times[-k, k]}\left<\mathbf{f_1(x_1 + o), f_2(x_2 + o)})\right>
$$

其中$<\cdot>$表示向量的点积, 向量的长度就是特征图的通道数.

<div class="polaroid-small">
    <img class="cool-img" src="/images/2018-12/FlowNet-3.jpg" FlowNet/>
    <div class="container">
        <p>末尾的 Refinement(Decoder) 结构</p>
    </div>
</div>

Refinment部分是接在前面两个网络的末尾用于增大光流图像的分辨率, 方法是反卷积, 网络最后输出图像的边长是原图的$1/4$, 最后直接插值回原始图像大小. 最后两次上采样用插值代替因为实验效果最好.

但实际上最终的两次上采样是通过[variational refinement](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.5756&rep=rep1&type=pdf)实现的. 这样虽然相比双线性插值增大了计算量, 但是能够得到变分法的好处: 分割更加光滑且准确. 

网络的输出是光流场, 实际上是一个大小为`[h/4, w/4, 2]`的向量场, 每个点都预测当前点的光流$(u, v)$, 损失函数为端点误差(endpoint error, EPE):

$$
error = \sqrt{(u - u_{GT})^2 + (v - v_{GT})^2}
$$

其中$(u_{GT}, v_{GT})$是ground truth.


## FlowNet 2.0

[原文链接](https://arxiv.org/pdf/1612.01925)

### Introduction

继承了FlowNet的优势:
* 能处理大的位移
* 在光流场中对小的细节的估计
* 对具体的场景可以学习潜在的先验
* 快的运行时间

FlowNet 2.0有其额外的优势:
* 能处理小的位移
* 解决了光流场中的噪声artifacts的问题

FlowNet 2.0 的贡献:
1. Dataset Schedule: 评估了数据集排序的影响
2. 引入了一个变形操作, 并说明了使用该操作stack多个网络能够显著提高表现
3. 针对小的位移和现实世界的数据给出了解决方案

### Dataset Schedule

这一部分讨论了同时有多个数据集可训练时, **数据集的训练次序是重要的**. 作者*推测*简单的数据集有利于网络学习颜色匹配的概念而不被复杂的属性混淆, 因此先用简单的数据集训练, 再使用复杂的数据集训练可以得到更好的效果.

