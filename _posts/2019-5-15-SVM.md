---
layout: post
title: "SVM 原理与算法"
data: 2019-5-15 16:10:00
categories: 机器学习
mathjax: true
figure: /images/2019-5/SVM-3_1.png
author: Jarvis
meta: Post
---

* content
{:toc}

> 支持向量机 (support vector machine, SVM) 是一类有监督地对二元数据分类的广义线性分类器, 其决策边界是对学习样本求解的最大间隔超平面. 




问题来源: 超平面 (hyperplane) 分类器的**不唯一性**. 

<div class="polaroid-script-less">
    <img class="cool-img" src="/images/2019-5/SVM-0.png" SVM/>
    <div class="container">
        <p>超平面分类器的不唯一性</p>
    </div>
</div>

因此需要给定一个**准则**来选择一个**最优**的超平面. 
<p hidden> $\def\xx{\mathbf{x}} \def\ww{\mathbf{w}} \def\vw{\lVert\ww\rVert} \def\sui{\sum_{i=1}^n} \def\suj{\sum_{j=1}^n}$ </p>

## 1. 线性 SVM 问题构建

### 1.1 几何间距

令 $\gamma$ 表示空间一点 $\xx$ 到超平面

$$
\ww^T\xx+b=0 \label{eq:hyperplane}
$$


的距离, $\ww$ 为超平面的法向. 设 $\xx$ 正投影到超平面的点为 $\xx_0$ , 则有

$$
\xx=\xx_0+y\cdot\gamma\frac{\ww}{\vw}.
$$

其中 $y$ 取值为 $1$ 或 $-1$, 表示 $\xx$ 在超平面的上方/下方. 如下图所示.

<div class="polaroid-script-less">
    <img class="cool-img" src="/images/2019-5/SVM-1.png" SVM/>
    <div class="container">
        <p>几何距离</p>
    </div>
</div>

### 1.2 最大化间隔

因为 $\xx_0$ 落在超平面上, 所以代入公式 $\eqref{eq:hyperplane}$ 得到

$$
\ww^T(\xx-y\cdot\gamma\frac{\ww}{\vw})+b=0.
$$

解得

$$
\gamma = y\frac{\ww^T\xx+b}{\vw}.
$$

当超平面在小范围内改变时, 超平面附近点的分类可能会发生变化, 而离超平面远的点的分类不会发生变化. 因此我们要寻找一个尽可能在两类点"中间"的一个超平面(即既要把两类点分开, 还要离两类点尽可能的远), 如下图中红色的线. 这种准则称为最大化间隔 (maximum margin).

<div class="polaroid-script-less">
    <img class="cool-img" src="/images/2019-5/SVM-2.png" SVM/>
    <div class="container">
        <p>最大化间隔</p>
    </div>
</div>

由此我们可以得到一个最大化间隔的分类器:

$$
\max_{\ww, b}\gamma=\max_{\ww,b}\frac{y(\ww^T\xx+b)}{\vw}\quad s.t., \gamma_i\leq\gamma,\quad i\in \{1,\cdots, n\},
$$

其中 $\gamma_i$ 表示训练集中第 $i$ 个点到超平面的距离. 由于分子项 $y(\ww^T\xx+b)$ 可以在不改变超平面的前提下任意变大, 因此为了方便我们固定 $\gamma\vw=y(\ww^T\xx+b)=1$ . 从而这个问题就变为了

$$
\min_{\ww, b}\vw,\quad s.t. \gamma_i=\frac{y_i(\ww^T\xx_i+b)}{\vw}\geq\gamma\quad i\in \{1,\cdots, n\}.
$$

即

$$
\min_{\ww,b}\vw,\quad s.t.~y_i(\ww^T\xx_i+b)\geq\gamma\vw=1.
$$

### 1.3 支持向量

最后我们的分类器可以表示为:

$$
\min_{\ww,b}\frac12\vw^2,\quad s.t.~y_i(\ww^T\xx_i+b)\geq1\quad i\in\{1,\cdots,n\}, \label{eq:classifier}
$$

其中目标函数增加 $1/2$ 和平方是为了优化的方便, 其最小值不会因此而改变. 注意到公式 $\eqref{eq:classifier}$ 中的不等式约束, 离目标超平面远的点的约束其实是不必要的, 因为他们会被离超平面更近点的约束所包含, 因此实际起作用的约束所对应的点是我们关心的, 这些点称为**支持向量 (support vector)**, 如下图所示, 蓝色圈中的点为支持向量, 这三个点对应的不等式约束是实际起作用的.

<div class="polaroid-script-less">
    <img class="cool-img" src="/images/2019-5/SVM-3.png" SVM/>
    <div class="container">
        <p>支持向量</p>
    </div>
</div>

### 1.4 松弛变量

前述模型在遇到离群点或错误点时, 会严重影响分类超平面的位置, 为了减弱这种影响, 为每个数据点引入一个松弛变量 (slack variable) $\xi_i\geq0$ , 把公式 $\eqref{eq:classifier}$ 不等式约束放松:

$$
y_i(\ww^T\xx_i+b)\geq1-\xi_i\quad i\in\{1,\cdots,n\}, \label{eq:cla22}
$$

但我们不希望松弛变量太大, 否则原始约束变得无效, 因此把松弛变量也加入目标函数  

$$
\min_{\ww,b}\frac12\vw^2+C\sui\xi_i, \label{eq:cla21}
$$

其中 $C$ 为控制了松弛变量的权重.

注意: 1.3和1.4中的两个优化问题均为**凸二次规划 (convex quadratic programming)**问题.

## 2. 线性 SVM 问题求解

### 2.1 求解原问题

Python 上的凸优化包 [CVXOPT](http://cvxopt.org) 

### 2.2 求解对偶问题

使用 Language 乘子法, 优化问题 $\eqref{eq:classifier}$ 可以变为

$$
L(\ww, b, \alpha)=\frac12\vw^2 - \sui\alpha_i(y_i(\ww^T\xx_i+b) - 1),
$$

其中 $\alpha=(\alpha_1,\cdots,\alpha_n)^T$ 为 Lagrange 乘子向量. 

> **定理:** 最优化问题
>
> $$
> \begin{align}
> \min_{\alpha} &\quad-\frac12\sui\suj y_iy_j\cdot\xx_i^T\xx_j\cdot\alpha_i\alpha_j+\suj\alpha_j, \\
> \text{s.t.} &\quad\sui y_i\alpha_i=0,\\
> &\quad\alpha_i\geq0,\;i=1,\cdots, n
> \end{align}
> $$
>
> 是原始问题 $\eqref{eq:classifier}$ 的对偶问题. 这里可以用极小化代替目标函数的负数的极大化.

最后可以根据对偶问题的解 $\alpha^*$ 计算原始问题的解:

$$
\begin{align}
\ww^* &= \sui\alpha_i^*y_i\xx_i, \\
b^* &= y_j - \sui\alpha^*_iy_i\xx_i^T\xx_j.
\end{align}
$$

得到分类器:

$$
f(\xx) = sign(\ww^{*T}\xx+b).
$$

### 2.3 带松弛变量的对偶问题

使用 Language 乘子法, 带松弛变量的优化问题 $\eqref{eq:cla22}\eqref{eq:cla21}$ 可以变为

$$
L(\ww, b, \xi, \alpha, \beta)=\frac12\vw^2 + C\sui\xi_i - \sui\alpha_i(y_i(\ww^T\xx_i+b) - 1 + \xi_i) - \sui\beta_i\xi_i,
$$

其中 $\alpha=(\alpha_1,\cdots,\alpha_n)^T,~\beta=(\beta_1,\cdots,\beta_n)^T$ 均为 Lagrange 乘子向量. 

> **定理:** 最优化问题
>
> $$
> \begin{align}
> \max_{\alpha}\quad &-\frac12\sui\suj y_iy_j\cdot\xx_i^T\xx_j\cdot\alpha_i\alpha_j+\suj\alpha_j, \\
> \text{s.t.}\quad &\sui y_i\alpha_i=0,\\
> &C - \alpha_i - \beta_i = 0,~i = 1,\cdots, n, \\ 
> &\alpha_i\geq0,\;i=1,\cdots, n, \\
> &\beta_i\geq0,\;i=1,\cdots, n \\
> \end{align}
> $$
>
> 是原始问题 $\eqref{eq:cla22}\eqref{eq:cla21}$ 的对偶问题. 

上面的定理可以消去 $\beta$ 而简化为

$$
\begin{align}
\min_{\alpha}\quad &\frac12\sui\suj y_iy_j\cdot\xx_i^T\xx_j\cdot\alpha_i\alpha_j-\suj\alpha_j, \\
\text{s.t.}\quad &\sui y_i\alpha_i=0,\\
&0\leq\alpha_i\leq C,~i=1,\cdots, n. \\
\end{align}
$$

## 3. SVM 的核方法

核方法可以把普通的线性 SVM 推广到非线性. 核方法的直观来自于数据点升维. 如下图中二维的点. 

<div class="polaroid-script-less">
    <img class="cool-img" src="/images/2019-5/SVM-4.png" SVM/>
    <div class="container">
        <p>非线性分类器</p>
    </div>
</div>

线性分类器无法得到圆形的分类边界, 但是如果通过某个函数 $\Phi(\cdot)$ 把点映射到三维空间(比如映射成锥形, 中间点的值较小, 周围点的值较大), 那么就可以找到一个三维空间中的二维平面把两类点分开.  如此我们只需要把第1节和第2节中所有的 $\xx$ 替换为 $\Phi(\xx)$ , 其他保持不变即可得到非线性 SVM 对应的推导.

### 3.1 核函数

可以发现在求解对偶问题时, $\Phi(\xx_i)\cdot\Phi(\xx_j)$ 总是以内积的形式出现, 因此我们可以把这样的形式定义为一个关于 $\xx_i$ 和 $\xx_j$ 的二元函数 $K(\cdot,\cdot)$

$$
K(\xx_i,\xx_j) = \Phi(\xx_i)\cdot\Phi(\xx_j),
$$

这样我们仍然可以得到相同的决策函数而简化了计算方式以及 $\Phi(\cdot)$ 函数的构造. 

核函数的种类 $\def\kk{K(x_1,x_2)}$:

* $\kk = (x_1\cdot x_2)$ 是核函数
* $f(\cdot)$ 是定义在 $R^n$ 上的实值函数, 则 $\kk = f(x_1)f(x_2)$ 是核函数
* 核函数的和与积都是核函数
* 若 $K_1(x_1, x_2)$ 是 $R^m\times R^m$ 上的核函数, $\theta(x)$ 是从 $R^n$ 到 $R^m$ 上的映射, 则 $K_2(x_1, x_2) = K_1(\theta(x_1), \theta(x_2))$ 是 $R^n$ 到 $R^n$ 上的核函数. 特别的, 若矩阵 $B$ 半正定, 则 $\kk = x_1^TBx_2$ 是 $R^n$ 到 $R^n$ 上的核函数.

常用的核函数:

* 多项式核函数 $\kk = (x_1\cdot x_2)^d$ 和 $\kk = (x_1\cdot x_2 + 1)^d$ 
* Gauss 径向基函数 $\kk = \exp\left(\frac{-\lVert x_1-x_2\rVert^2}{\sigma^2}\right)$ 

### 3.2 C - 支持向量分类机

使用步骤:

1. 选择核函数 $K(\cdot,\cdot)$ 和惩罚参数 $C\geq0$ 

2. 求解优化问题:
    
   $$
   \begin{align}
   \min_{\alpha}\quad &\frac12 \sui \suj y_iy_j\cdot K(\xx_i,\xx_j)\cdot\alpha_i\alpha_j- \suj \alpha_j, \\
   \text{s.t.}\quad & \sui y_i\alpha_i=0, \\
   &0\leq\alpha_i\leq C,~i=1,\cdots,n,
   \end{align}
   $$
   
   的解为 $\alpha^\*=(\alpha_1^\*,\cdots,\alpha^\*_n)$ .

3. 计算 $b^\*$ , 选择位于区间 $(0, C)$ 的 $\alpha^\*$ 的分量 $\alpha^\*_j$ , 据此计算
   
   $$
   b^* = y_j - \sui y_i\alpha_i^*K(\xx_i, \xx_j);
   $$

4. 构造决策函数

   $$
   f(\xx) = sign(\sui y_i\alpha_iK(\xx_i, \xx) + b^*).
   $$

5. 获得支持向量* : 支持向量为 $\\{(\xx_j, y_j)\lvert \alpha^*_j=0\\}$ .

## 4. Appendix*

* SVM 函数库: [LibSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)
