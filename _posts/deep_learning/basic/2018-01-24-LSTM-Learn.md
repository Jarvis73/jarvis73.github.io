---
layout: post
title: å¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Network, RNN)
date: 2018-01-24 19:03:00 +0800
update: 2019-11-18
categories: æ·±åº¦å­¦ä¹ 
mathjax: true
figure: /images/2018/01/LSTM.png
author: Jarvis
meta: Post
---

* content
{:toc}





## 1. å¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Network, RNN)

åœ¨æœºå™¨å­¦ä¹ ä¸­, æ•°æ®è¡¨ç¤ºä¸º $$ n $$ ç»´ç‰¹å¾å‘é‡ $$ \mathbf{x}\in\mathbb{R}^n $$ æˆ– $$ h\times w $$ ç»´ç‰¹å¾çŸ©é˜µ(å¦‚å›¾ç‰‡), å¤šå±‚æ„ŸçŸ¥æœº (multilayer perceptron, MLP) å’Œå·ç§¯ç¥ç»ç½‘ç»œ (convolutional neural network, CNN) å¯ä»¥æå–æ•°æ®ä¸­çš„ç‰¹å¾ä»¥è¿›è¡Œåˆ†ç±»å›å½’ç­‰ä»»åŠ¡.
ä½†é€šå¸¸çš„ MLP æˆ– CNN å¤„ç†çš„æ•°æ®é€šå¸¸è®¤ä¸ºæ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„, å› æ­¤å½“æ•°æ®ä¹‹é—´å­˜åœ¨å…³è”å…³ç³»æ—¶, è¿™ç±»æ¨¡å‹åˆ™æ— æ³•å¾ˆå¥½çš„ç¼–ç æ•°æ®é—´çš„ä¾èµ–å…³ç³», å¯¼è‡´æ¨¡å‹çš„è¡¨ç°è¾ƒå·®. ä¸€ç§å…¸å‹çš„æ•°æ®é—´ä¾èµ–å…³ç³»å°±æ˜¯*æ—¶åºå…³ç³»*. æ¯”å¦‚è¯è¯´ä¸€åŠæ—¶å¯¹æ–¹å¯èƒ½å°±çŸ¥é“äº†ä½ çš„æ„æ€, ä¸€å¥è¯ä¸­çš„ä»£è¯"ä»–"æŒ‡ä»£çš„ç›®æ ‡éœ€è¦åˆ†æä¸Šä¸‹æ–‡åæ‰èƒ½å¾—åˆ°. æ—¶åºæ•°æ®å¦‚ä¸‹å›¾æ‰€ç¤º, ä¸€åˆ—å‘é‡åˆ™è¡¨ç¤ºä¸€ä¸ªå­—çš„ç¼–ç .

{% include image.html class="polaroid" url="2018/01/time-series.png" title="æ—¶åºæ•°æ®" %}

å¾ªç¯ç¥ç»ç½‘ç»œçš„æå‡ºå°±æ˜¯ä¸ºäº†è§£å†³æ•°æ®ä¸­è¿™ç§å…¸å‹çš„æ—¶åºä¾èµ–å…³ç³». RNN æ˜¯å†…éƒ¨åŒ…å«å¾ªç¯çš„ç¥ç»ç½‘ç»œ (æ™®é€š CNN ä¸åŒ…å«å¾ªç¯), RNN çš„ä¸€ä¸ªå¾ªç¯å•å…ƒå¦‚ä¸‹å›¾æ‰€ç¤º[^1].

{% include image.html class="polaroid" url="2018/01/RNN-rolled.png" title="RNN çš„å¾ªç¯å•å…ƒ" %}

å…¶ä¸­ $$ x_t $$ è¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­æ—¶åˆ» $$ t $$ æ—¶çš„å€¼, $$ h_t $$ ä¸ºè¯¥å±‚åœ¨æ—¶åˆ» $$ t $$ çš„è¾“å‡º. æ–¹å— $$ A $$ æ˜¯ä¸€ä¸ªæ“ä½œç¬¦, æŠŠå‰ä¸€æ—¶åˆ»çš„è¾“å‡º $$ h_{t-1} $$ å’Œå½“å‰æ—¶åˆ»çš„è¾“å…¥ $$ x_t $$ æ˜ å°„ä¸ºå½“å‰æ—¶åˆ»çš„è¾“å‡º. æ³¨æ„, $$ h_t $$ é€šå¸¸æ‰®æ¼”ä¸¤ä¸ªè§’è‰², æ—¢æ˜¯å¾ªç¯å•å…ƒåœ¨å½“å‰æ—¶åˆ»çš„è¾“å‡º, åˆæ˜¯å½“å‰æ—¶åˆ»å¾ªç¯å•å…ƒçš„*çŠ¶æ€*. å…¬å¼è¡¨ç¤ºå¦‚ä¸‹:

$$
h_t = \sigma(W_{hx}x_t + W_{hh}h_{t-1}).
$$

å…¶ä¸­ $$ W $$ ä¸ºæƒé‡å‚æ•°, $$ \sigma $$ è¡¨ç¤ºæ¿€æ´»å‡½æ•°, å¸¸ç”¨çš„æ˜¯ $$ \tanh(\cdot) $$ å‡½æ•°, å¯ä»¥æŠŠè¾“å‡ºçš„å€¼åŸŸæ§åˆ¶åœ¨ $$ [-1, 1] $$ ä¹‹é—´, é¿å…åœ¨å¾ªç¯è¿‡ç¨‹ä¸­ä¸æ”¶æ•›. æˆ‘ä»¬å¯ä»¥æ²¿ç€æ—¶é—´è½´æŠŠä¸Šé¢çš„å¾ªç¯å•å…ƒå±•å¼€, æ›´åŠ ç›´è§‚.

{% include image.html class="polaroid" url="2018/01/RNN-unrolled.png" title="RNN å¾ªç¯å•å…ƒå±•å¼€ç¤ºæ„å›¾" %}

å¾ªç¯ç¥ç»ç½‘ç»œå¯ä»¥ç”±å¤šå±‚å¾ªç¯å•å…ƒå †å è€Œæˆ, å‰ä¸€ä¸ªå¾ªç¯å•å…ƒçš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å¾ªç¯å•å…ƒçš„è¾“å…¥, å¦‚ä¸‹å›¾æ‰€ç¤º. 
<!-- åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­, ... -->

{% include image.html class="polaroid" url="2018/01/multilayer-RNN.png" title="å¤šå±‚å¾ªç¯å•å…ƒå †å " %}

RNN çš„è¾“å…¥é€šå¸¸è¡¨ç¤ºæˆ**åµŒå…¥ (embedding)**çš„å½¢å¼, å³æ„é€ ä¸€ä¸ª**æŸ¥è¯¢è¡¨ (lookup table)**, æŠŠè¾“å…¥åºåˆ—çš„æ¯ä¸ªæ—¶åˆ»çš„ç‰¹å¾å‘é‡é€šè¿‡æŸ¥è¯¢è¡¨è½¬ä¸ºä¸€ä¸ªç­‰é•¿çš„å‘é‡. ä»è€Œä¸€ä¸ªåºåˆ—çš„å½¢çŠ¶å˜ä¸º `[num_time_steps, embedding_size]`. 

### 1.1 RNN çš„åº”ç”¨

RNN å¯ä»¥æ ¹æ®è¾“å…¥åºåˆ—çš„é•¿åº¦å’Œè¾“å‡ºåºåˆ—çš„é•¿åº¦åˆ†ä¸ºä¸‰å¤§ç±».

* å¤šå¯¹ä¸€: å¸¸ç”¨äºæƒ…æ„Ÿåˆ†æ, æ–‡æœ¬åˆ†ç±»
* ä¸€å¯¹å¤š: Image Caption
* å¤šå¯¹å¤š: æœºå™¨ç¿»è¯‘
* ä¸€å¯¹ä¸€: é€€åŒ–ä¸º MLP

### 1.2 RNN çš„å±€é™æ€§

RNN ä¹Ÿå­˜åœ¨ä¸€äº›ç¼ºé™·:

* RNN å¯ä»¥å¾ˆå¥½çš„å­¦ä¹ åºåˆ—ä¸­é‚»è¿‘æ—¶é—´æ­¥æ•°æ®ç‚¹(çŸ­æœŸ)ä¹‹é—´çš„å…³ç³», ä½†å¯¹äºé•¿æœŸä¾èµ–ä¼šå˜å¾—ä¸ç¨³å®š.
* RNN å¯ä»¥æŠŠå›ºå®šé•¿åº¦çš„è¾“å…¥åºåˆ—æ˜ å°„åˆ°æŒ‡å®šé•¿åº¦çš„è¾“å‡ºåºåˆ—, ä½†ä¸èƒ½åŠ¨æ€åœ°æ ¹æ®è¾“å…¥å†³å®šè¾“å‡ºå¤šé•¿çš„åºåˆ—.

è€Œ LSTM å’Œ Encoder-Decoder çš„æå‡ºè§£å†³äº†è¿™ä¸¤ä¸ªé—®é¢˜.


## 2. é•¿çŸ­æœŸè®°å¿† (Long Short Term Memory, LSTM)[^1]

å‰é¢æåˆ°, RNN å¯¹äºé•¿æœŸä¾èµ–ç»å®éªŒè¡¨æ˜æ˜¯ä¸ç¨³å®šçš„. å¯¹äºçŸ­åºåˆ—, å¦‚ä¸€ä¸ªå¥å­: "The clouds are in the ()", æ‹¬å·ä¸­é¢„æµ‹ä¸€ä¸ªè¯, é‚£ä¹ˆå¾ˆå®¹æ˜“æ ¹æ®è¯¥è¯å‰é¢çš„ clouds å’Œ in æ¨æ–­å‡ºå¡« sky. ä½†æ˜¯å¯¹äºé•¿åºåˆ—, å¦‚ "I grew up in France ... I speak fluent ()", å¥å­ä¸­çš„çœç•¥å·åŒ…å«äº†å¤§é‡å…¶ä»–ä¿¡æ¯, æ­¤æ—¶æœ€åæ‹¬å·ä¸­çš„è¯åº”å½“æ ¹æ®å¼€å¤´çš„ France æ¨æ–­ä¸º French, ä½†ä¸­é—´å¤§é‡çš„æ— ç”¨è¯­å¥ä¼šç¨€é‡Šå‰æœŸçš„ä¿¡æ¯, å¯¼è‡´ RNN æ— æ³•æ­£ç¡®é¢„æµ‹æœ€åçš„è¯. è€Œ Hochreiter & Schmidhuber æå‡ºçš„ LSTM [^6] æ­£æ˜¯è§£å†³è¯¥é—®é¢˜çš„. 

LSTM å’Œé€šå¸¸çš„ CNN ä¸€æ ·ä¸ºä¸€ä¸ªå¾ªç¯å•å…ƒçš„ç»“æ„, ä½†æ˜¯ä¸ RNN ä»…æœ‰ä¸€ä¸ª tanh æ¿€æ´»å±‚ä¸åŒ, LSTM ä¸­åŒ…å«äº†æ›´å¤æ‚çš„å››å±‚ç½‘ç»œçš„ç»“æ„è®¾è®¡, å¹¶ä¸”å››å±‚ç½‘ç»œç›¸äº’è€¦åˆ, å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/LSTM.png" title="LSTM å¾ªç¯å•å…ƒå±•å¼€ç¤ºæ„å›¾" %}

ä¸Šå›¾ä¸­çš„åœ†è§’çŸ©å½¢æ¡†, æ“ä½œç¬¦, åˆ†æ”¯ç®­å¤´çš„å«ä¹‰å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/LSTM2-notation.png" title="å›¾ä¾‹" %}

ä¸‹é¢è¯¦ç»†ä»‹ç» LSTM å•å…ƒçš„å†…éƒ¨ç»“æ„. 

### 2.1 LSTM çš„æ ¸å¿ƒæ€æƒ³

LSTM ç›¸æ¯”äº RNN, å…³é”®åœ¨äºå¼•å…¥äº†å•å…ƒçŠ¶æ€(state) $$ C $$ â€”â€” æ¨ªç©¿ä¸‹å›¾é¡¶éƒ¨çš„ç›´çº¿. 

{% include image.html class="polaroid" url="2018/01/LSTM3-C-line.png" title="å•å…ƒçŠ¶æ€" %}

LSTM å¯ä»¥é€šè¿‡**é—¨(gate)**æ¥æ§åˆ¶å‘å•å…ƒçŠ¶æ€ä¸­å¢åŠ ä¿¡æ¯æˆ–å‡å°‘ä¿¡æ¯. é—¨ç”±ä¸€ä¸ª $$ sigmoid $$ å‡½æ•°å’Œä¸€ä¸ªä¹˜æ³•è¿ç®—ç¬¦ç»„æˆ, å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/LSTM3-gate.png" title="é—¨" %}

$$ sigmoid $$ å±‚è¾“å‡ºçš„å€¼åœ¨ $$ [0, 1] $$ ä¹‹é—´, æ§åˆ¶äº†ä¿¡æ¯çš„é€šè¿‡é‡. è¶Šæ¥è¿‘ 0, åˆ™è¡¨æ˜ä¸å…è®¸ä¿¡æ¯é€šè¿‡(ä»è€Œå½¢æˆ*é—å¿˜*); è¶Šæ¥è¿‘ 1, åˆ™è¡¨æ˜å…è®¸ä¿¡æ¯å…¨éƒ¨é€šè¿‡(ä»è€Œå½¢æˆ*è®°å¿†*). 

### 2.2 LSTM å•å…ƒè§£æ

LSTM å•å…ƒåœ¨æ¯ä¸ªæ—¶é—´æ­¥éœ€è¦æ³¨æ„ä¸‰ä¸ªå‘é‡: 
* è¾“å…¥çš„ç‰¹å¾å‘é‡ $$ x_t $$
* ä¸Šä¸€æ­¥è¾“å‡ºçš„ç‰¹å¾å‘é‡ $$ h_{t-1} $$
* ä¸Šä¸€æ­¥ç»“æŸåçš„å•å…ƒçŠ¶æ€ $$ C_{t-1} $$

è¦æ³¨æ„ä¸‰ä¸ªå‘é‡æ˜¯ç›¸åŒçš„é•¿åº¦.

**é—å¿˜é—¨(forget gate).** æ¯å¾ªç¯ä¸€æ­¥æ—¶, é¦–å…ˆæ ¹æ®ä¸Šä¸€æ­¥çš„è¾“å‡º $$ h_{t-1} $$ å’Œå½“å‰æ­¥çš„è¾“å…¥ $$ x_t $$ æ¥å†³å®šè¦é—å¿˜æ‰ä¸Šä¸€æ­¥çš„ä»€ä¹ˆä¿¡æ¯(ä»å•å…ƒçŠ¶æ€ $$ C_{t-1} $$ ä¸­é—å¿˜). å› æ­¤åªéœ€è¦è®¡ç®—ä¸€ä¸ªé—å¿˜ç³»æ•° $$ f_t $$ ä¹˜åˆ°å•å…ƒçŠ¶æ€ä¸Šå³å¯. å¦‚ä¸‹å›¾æ‰€ç¤º, å…¬å¼ä¸­çš„æ–¹æ‹¬å·è¡¨ç¤º $$ concat $$ æ“ä½œ.

{% include image.html class="polaroid" url="2018/01/LSTM3-focus-f.png" title="é—å¿˜é—¨" %}

**è¾“å…¥é—¨(input gate).** è¿™ä¸€æ­¥æ¥å†³å®šå½“å‰æ–°çš„è¾“å…¥ $$ x_t $$ æˆ‘ä»¬åº”è¯¥æŠŠå¤šå°‘ä¿¡æ¯å‚¨å­˜åœ¨å•å…ƒçŠ¶æ€ä¸­. è¿™éƒ¨åˆ†æœ‰ä¸¤æ­¥, é¦–å…ˆä¸€ä¸ªè¾“å…¥é—¨è®¡ç®—è¦ä¿ç•™å“ªäº›ä¿¡æ¯, å¾—åˆ°è¿‡æ»¤ç³»æ•° $$ i_t $$, ç„¶åä½¿ç”¨ä¸€ä¸ªå…¨è¿æ¥å±‚æ¥ä»ä¸Šä¸€æ­¥çš„è¾“å‡º $$ h_{t-1} $$ å’Œå½“å‰æ­¥çš„è¾“å…¥ $$ x_t $$ ä¸­æå–ç‰¹å¾ $$ \tilde{C}_t $$. å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/LSTM3-focus-i.png" title="è¾“å…¥é—¨" %}

**æ–°æ—§ä¿¡æ¯åˆå¹¶.** è®¡ç®—å¥½äº†é—å¿˜ç³»æ•°, è¾“å…¥ç³»æ•°, ä»¥åŠæ–°çš„è¦è®°å¿†çš„ç‰¹å¾, ç°åœ¨å°±å¯ä»¥åœ¨å•å…ƒçŠ¶æ€ $$ C_{t-1} $$ ä¸Šæ‰§è¡Œé—å¿˜æ“ä½œ $$ f_t\ast C_{t-1} $$ å’Œè®°å¿†æ“ä½œ $$ +i_t\ast\tilde{C}_t $$. å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/LSTM3-focus-C.png" title="æ–°æ—§ä¿¡æ¯åˆå¹¶" %}

**è¾“å‡ºé—¨(output gate).** æœ€åæˆ‘ä»¬è¦å†³å®šè¾“å‡ºä»€ä¹ˆä¿¡æ¯äº†. è¿™éœ€è¦ä»å½“å‰çš„å•å…ƒçŠ¶æ€ $$ C_t $$ æ¥è·å–è¦è¾“å‡ºçš„ä¿¡æ¯. ä½†æ˜¾ç„¶æˆ‘ä»¬å¹¶ä¸ä¼šåœ¨è¿™ä¸€ä¸ªæ—¶é—´æ­¥è¾“å‡ºæ‰€æœ‰è®°å¿†çš„ä¿¡æ¯, è€Œæ˜¯åªè¦è¾“å‡ºå½“å‰éœ€è¦çš„ä¿¡æ¯, å› æ­¤æˆ‘ä»¬ç”¨ä¸€ä¸ªè¾“å‡ºé—¨æ¥è¿‡æ»¤è¾“å‡ºçš„ä¿¡æ¯, è¿‡æ»¤ç³»æ•°ä¸º $$ o_t $$. æ­¤å¤–æˆ‘ä»¬å¸Œæœ›è¾“å‡ºçš„ç‰¹å¾çš„å–å€¼èƒ½å¤Ÿä»‹äº $$ [-1, 1] $$ ä¹‹é—´, å› æ­¤ä½¿ç”¨ä¸€ä¸ª $$ tanh $$ å‡½æ•°æŠŠå•å…ƒçŠ¶æ€ $$ C_t $$ æ˜ å°„åˆ°ç›¸åº”çš„èŒƒå›´, æœ€åä¹˜ä¸Šè¿‡æ»¤ç³»æ•°å¾—åˆ°å½“å‰æ­¥çš„è¾“å‡º. å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/LSTM3-focus-o.png" title="è¾“å‡ºé—¨" %}

### 2.3 LSTM å•å…ƒå˜ä½“

**å˜ä½“ä¸€.** [^7]è®©æ‰€æœ‰çš„é—¨æ§å•å…ƒåœ¨è¾“å‡ºé—¨æ§ç³»æ•°çš„æ—¶å€™éƒ½å¯ä»¥"çœ‹åˆ°"å½“å‰çš„å•å…ƒçŠ¶æ€. å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/LSTM3-var-peepholes.png" title="è®©é—¨æ§å•å…ƒå¯ä»¥çœ‹åˆ°å•å…ƒçŠ¶æ€" %}

**å˜ä½“äºŒ.** è®©é—å¿˜é—¨çš„é—å¿˜ç³»æ•° $$ f_t $$ å’Œè¾“å…¥é—¨çš„è¾“å…¥ç³»æ•° $$ i_t $$ è€¦åˆ, å³ä»¤ $$ i_t = 1-f_t $$, ä»è€ŒåŒæ—¶åšå‡ºå“ªäº›ä¿¡æ¯é—å¿˜ä»¥åŠå“ªäº›ä¿¡æ¯è®°å¿†çš„å†³ç­–. è¿™ä¸ªå˜ä½“å¯ä»¥è®©æ–°çš„æœ‰ç”¨çš„è®°å¿†"è¦†ç›–"è€çš„æ— ç”¨çš„è®°å¿†. å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/LSTM3-var-tied.png" title="é—å¿˜ç³»æ•°å’Œè®°å¿†ç³»æ•°è€¦åˆ" %}

**å˜ä½“ä¸‰(GRU).** [^8]ç¬¬ä¸‰ç§å˜ä½“æ›´ä¸ºæœ‰å, ç§°ä¸º**é—¨æ§å¾ªç¯å•å…ƒ(Gated Recurrent Unit, GRU)**. ä¸‹ä¸€èŠ‚ä»‹ç».

**å…¶ä»–å‚è€ƒ:** å…¶ä»–å¯ä»¥å‚è€ƒå¦‚ä¸‹æ–‡çŒ®:

* [Depth Gated RNNs](http://arxiv.org/pdf/1508.03790v2.pdf)
* [Variants comparision](http://arxiv.org/pdf/1503.04069.pdf)
* [Ten thousand RNN architecture tests](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)

## 3. é—¨æ§å¾ªç¯å•å…ƒ(Gated Recurrent Unit, GRU)

GRU[^8] æ˜¯ä¸€ç§æ¯” LSTM ç¨ç®€å•ä¸€äº›çš„å¾ªç¯å•å…ƒ. GRU æŠŠ LSTM ä¸­çš„éšè—çŠ¶æ€ $$ h $$ å’Œå•å…ƒçŠ¶æ€ $$ C $$ åˆå¹¶ä¸ºå•ä¸ªçš„éšè—çŠ¶æ€. å¦‚ä¸‹å›¾æ‰€ç¤º. 

{% include image.html class="polaroid" url="2018/01/LSTM3-var-GRU.png" title="é—¨æ§å¾ªç¯å•å…ƒ(GRU)" %}

**æ›´æ–°é—¨(update gate).** æ›´æ–°é—¨ç³»æ•° $$ z_t $$ æ§åˆ¶äº† $$ h_{t-1} $$ ä¸­ä¿å­˜çš„ä¿¡æ¯(å¦‚é•¿æœŸè®°å¿†)åœ¨å½“å‰æ­¥ä¿ç•™å¤šå°‘. $$ z_t $$ æ¥è¿‘ 0 æ—¶ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ä¸­çš„ä¿¡æ¯ $$ h_{t-1} $$ å¾—ä»¥ä¿ç•™, æ–°è¾“å…¥çš„ä¿¡æ¯ $$ \tilde{h}_t $$ ä¼šè¢«å¿½ç•¥; $$ z_t $$ æ¥è¿‘ 1 æ—¶åˆ™ä¸¢å¼ƒå·²æœ‰çš„ä¿¡æ¯, å¹¶å¡«å…¥æ–°è¾“å…¥çš„ä¿¡æ¯. 

**è¾“å…¥ä¿¡æ¯çš„åŠ å·¥.** å½“å‰æ­¥è¾“å…¥çš„ä¿¡æ¯éœ€è¦åŠ å·¥åæ‰èƒ½åˆå¹¶åˆ°éšè—çŠ¶æ€ $$ h_t $$ ä¸­. è¾“å…¥ä¿¡æ¯åŠ å·¥æ—¶éœ€è¦å‚è€ƒä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ $$ h_{t-1} $$ æ¥å†³å®šå“ªäº›ä¿¡æ¯æœ‰ç”¨, å“ªäº›æ²¡ç”¨. åŠ å·¥åçš„ä¿¡æ¯ç”¨ $$ \tilde{h}_t $$ è¡¨ç¤º.

**é‡ç½®é—¨(reset gate).** é‡ç½®é—¨ç³»æ•° $$ r_t $$ æ§åˆ¶äº†åœ¨åŠ å·¥è¾“å…¥ä¿¡æ¯çš„æ—¶å€™ä½¿ç”¨ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ä¸­çš„å“ªäº›ä¿¡æ¯. $$ r_t $$ æ¥è¿‘ 0 æ—¶æ–°è¾“å…¥çš„ä¿¡æ¯å ä¸»å¯¼åœ°ä½, è¯´æ˜å½“å‰æ­¥çš„è¾“å…¥åŒ…å«çš„ä¿¡æ¯ä¸å‰é¢çš„ä¿¡æ¯å…³è”æ€§å¾ˆå°; $$ r_t $$ æ¥è¿‘ 1 æ—¶æ–°è¾“å…¥çš„ä¿¡æ¯å’Œå‰é¢çš„é•¿æœŸä¿¡æ¯æœ‰è¾ƒå¤§å…³è”æ€§, éœ€è¦ç»¼åˆè€ƒè™‘æ¥äº§ç”Ÿå½“å‰æ­¥åŠ å·¥åçš„ä¿¡æ¯. 

## 4. ç¼–ç -è§£ç å™¨ (Encoder-Decoder)

ç¼–ç -è§£ç å™¨æ¨¡å‹æ˜¯ä¸ºäº†å®ç° $$ n\rightarrow m $$ åºåˆ—æ˜ å°„çš„æ¨¡å‹æ¡†æ¶. è¿™ç±»æ¨¡å‹ä¹Ÿç§°ä¸º Sequence to Sequence(seq2seq). ç¼–ç å™¨åªè´Ÿè´£å¤„ç†è¾“å…¥åºåˆ—, å¹¶å½¢æˆè¾“å…¥åºåˆ—çš„ç‰¹å¾å‘é‡åé€å…¥è§£ç å™¨. ä¸ºäº†æ¸…æ¥š, æˆ‘ä»¬ç”¨å…¬å¼æ¥è¡¨ç¤ºè¿™ä¸ªè¿‡ç¨‹[^5]. å¯¹äºè¾“å…¥åºåˆ— $$ X=\{x_1, x_2, \cdots, x_S\} $$ å’ŒæœŸæœ›çš„è¾“å‡ºåºåˆ— $$ Y=\{y_1, y_2, \cdots, y_T\} $$, æˆ‘ä»¬ä½¿ç”¨ RNN æ¨¡å‹å¯¹å…¶è¿›è¡Œå»ºæ¨¡, å½¢æˆä¸€ä¸ªæ¡ä»¶æ¦‚ç‡ $$ P(Y\vert X) $$, ä½¿ç”¨é“¾å¼æ³•åˆ™è§£è€¦å¦‚ä¸‹:

$$
P(Y|X) = \prod_{t=1}^TP(y_t\lvert y_1, y_2, \cdots, y_{t-1}, X).
$$

é‚£ä¹ˆè¯¥ RNN æ¨¡å‹å°±æ˜¯ä¸€ä¸ªç¼–ç å™¨, åœ¨æ—¶åˆ» $$ s $$ çš„çŠ¶æ€é€šè¿‡ä¸‹å¼è®¡ç®—:

$$
h_s = f_{enc}(h_{s-1}, x_s).
$$

ç¼–ç å™¨çš„ç¤ºæ„å›¾å¦‚ä¸‹å›¾[^2]æ‰€ç¤º:

{% include image.html class="polaroid" url="2018/01/encoder.png" title="ç¼–ç å™¨. è¾“å‡ºçš„ä¸­é—´çŠ¶æ€(è¯­ä¹‰å‘é‡) $$ c $$ å¯ä»¥é€šè¿‡ä¸åŒçš„è®¡ç®—å…¬å¼æ„é€ , å½¢æˆä¸åŒçš„æ¨¡å‹ç»“æ„." %}

è§£ç å™¨ RNN æ¯ä¸ªæ—¶é—´æ­¥ä½¿ç”¨å‰ä¸€æ­¥çš„è¾“å‡º $$ y_{t-1} $$ å’Œå½“å‰çŠ¶æ€ $$ g_t $$ äº§ç”Ÿä¸€ä¸ªè¾“å‡º $$ y_t\in Y $$:

$$
\begin{align}
g_1 &= h_s \\
g_t &= f_{dec}(g_{t-1}, y_{t-1})
\end{align}
$$

æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºæ¦‚ç‡é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚å’Œä¸€ä¸ª softmax å‡½æ•°å¾—åˆ°:

$$
P(y_t\lvert y_1, y_2, \cdots, y_{t-1}, X) = Softmax(Linear(g_t)).
$$

ä½¿ç”¨è§£ç å™¨å¯¹è¯­ä¹‰å‘é‡è§£ç . å¦‚æœæŠŠè¯­ä¹‰å‘é‡åªè¾“å…¥è§£ç å™¨çš„ç¬¬ä¸€ä¸ªå¾ªç¯æ—¶é—´æ­¥, åˆ™å½¢æˆäº† Cho et al.[^3] æå‡ºçš„ç»“æ„.

{% include image.html class="polaroid" url="2018/01/decoder-1.png" title="è§£ç å™¨-1" %}

å¦‚æœæŠŠè¯­ä¹‰å‘é‡åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½è¾“å…¥è§£ç å™¨, åˆ™å½¢æˆäº† Sutskever et al.[^4] æå‡ºçš„ç»“æ„.

{% include image.html class="polaroid" url="2018/01/decoder-2.png" title="è§£ç å™¨-2" %}

### 4.1 ç¼–ç -è§£ç å™¨æ¨¡å‹çš„å±€é™æ€§

* ä¿¡æ¯çš„ä¸¢å¤±: æ•´ä¸ªæ—¶é—´åºåˆ—åªèƒ½å‹ç¼©ä¸ºä¸€ä¸ªå›ºå®šé•¿åº¦çš„è¯­ä¹‰å‘é‡
* ä¸åˆç†æ€§: seq2seq çš„ä»»åŠ¡ä¸­è¾“å…¥åºåˆ— $$ \{x_0, x_1, \dots, x_{tâˆ’1},  x_ğ‘¡, x_{t+1},\dots \} $$ ä¸­çš„æ¯ä¸ªå…ƒç´ å¯¹æ‰€æœ‰ $$ y_s $$ çš„è´¡çŒ®åº¦æ˜¯ç›¸åŒçš„

ä¾‹å¦‚: The animal didn't cross the street because **it** was too tired. åœ¨è¿™å¥è¯ä¸­, äººæ˜¯é€šè¿‡ç»¼åˆæ•´å¥è¯çš„ä¿¡æ¯æ¥åˆ¤æ–­å•è¯ it æŒ‡ä»£çš„æ˜¯ the animal, ä»è€Œç¿»è¯‘æ—¶ the animal åº”è¯¥å¯¹ it çš„å½±å“æ›´å¤§.

äººä»¬æå‡ºäº†æ³¨æ„åŠ›æ¨¡å‹æ¥è§£å†³æ™®é€šç¼–ç -è§£ç å™¨æ¨¡å‹çš„é—®é¢˜.


## 5. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)[^11]

### 5.1 ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›?

å¿ƒç†å­¦ä¸­å¯¹æ³¨æ„åŠ›çš„è§£é‡Šæ˜¯:

> **Attention**Â is the behavioral andÂ cognitive processÂ of selectively concentrating on a discrete aspect of information, whether deemed subjective or objective, while ignoring other perceivable information.

æ³¨æ„åŠ›æ˜¯æŠŠæœ‰é™çš„èµ„æºé›†ä¸­åœ¨æ›´é‡è¦çš„ç›®æ ‡ä¸Š. æ³¨æ„åŠ›æœºåˆ¶çš„ä¸¤ä¸ªè¦ç´ :

* å†³å®šè¾“å…¥ä¿¡æ¯çš„å“ªéƒ¨åˆ†æ˜¯é‡è¦çš„
* æŠŠèµ„æºé›†ä¸­åˆ†é…åˆ°é‡è¦çš„ä¿¡æ¯ä¸Š

æ²¿ç”¨ç¼–ç -è§£ç å™¨æ¨¡å‹çš„ç»“æ„, æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡å¼•å…¥ä¸€ç»„å½’ä¸€åŒ–çš„ç³»æ•° $$ \{\alpha_1, \alpha_2, \dots, \alpha_n \} $$ æ¥å¯¹è¾“å…¥çš„ä¿¡æ¯è¿›è¡Œé€‰æ‹©, æ¥è§£å†³ç¼–ç -è§£ç å™¨çš„ä¸åˆç†æ€§. è¿™é‡Œè¾“å…¥çš„ä¿¡æ¯å°±æ˜¯æŒ‡è¾“å…¥åºåˆ—åœ¨ RNN ä¸­çš„å•å…ƒè¾“å‡º $$ \mathbf{h}_s $$. å½’ä¸€åŒ–çš„ç³»æ•° $$ \alpha_s $$ ç”¨æ¥å†³å®šè¾“å…¥ä¿¡æ¯çš„é‡è¦æ€§, æ˜¯ç¼–ç å™¨è¾“å‡ºæ—¶å¯¹å•å…ƒè¾“å‡ºåŠ æƒæ±‚å’Œç³»æ•°. æ³¨æ„åŠ›æœºåˆ¶åœ¨è®¡ç®—ä¸åŒæ—¶é—´æ­¥çš„è¾“å‡ºæ—¶, å®æ—¶æ„é€ ç¼–ç å™¨è¾“å‡ºçš„è¯­ä¹‰å‘é‡ $$ \mathbf{c}_t $$, ä»è€Œè§£å†³äº†æ™®é€šç¼–ç -è§£ç å™¨ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜. æ³¨æ„åŠ›æœºåˆ¶å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/attention-1.png" title="æ³¨æ„åŠ›æœºåˆ¶" %}

ä¸‹é¢è®¨è®ºåŠ æƒç³»æ•°æ˜¯å¦‚ä½•è®¡ç®—çš„. è€ƒè™‘åˆ°åŠ æƒç³»æ•°è¦åæ˜ **è¾“å…¥ä¿¡æ¯**åœ¨å½“å‰**æ—¶é—´æ­¥**ä¸Šçš„é‡è¦æ€§, å› æ­¤éœ€è¦æŠŠè¾“å…¥ä¿¡æ¯å’Œæ—¶é—´ä¿¡æ¯ç»“åˆèµ·æ¥è®¡ç®—åŠ æƒç³»æ•°. å› æ­¤é€šå¸¸ä½¿ç”¨ä¸€ä¸ª MLP æ¥æ”¶è¾“å…¥ä¿¡æ¯ $$ \mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_n $$ å’Œè§£ç å™¨ä¸Šä¸€æ—¶åˆ»çš„çŠ¶æ€ $$ \mathbf{s}_{t-1} $$ ä½œä¸ºè¾“å…¥, è¾“å‡ºå½’ä¸€åŒ–çš„åŠ æƒç³»æ•° $$ \alpha_{t1}, \alpha_{t2}, \dots, \alpha_{tn} $$. å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/attention-2.png" title="æ³¨æ„åŠ›æœºåˆ¶" %}

æ³¨æ„: ç¼–ç -è§£ç å™¨çš„ç»“æ„åªæ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ä¸ªè½½ä½“, æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒåœ¨äºåŠ æƒç³»æ•°, å› æ­¤å¯ä»¥ç”¨äºé RNN çš„ç»“æ„.

### 5.2 è‡ªæ³¨æ„åŠ› (Self-Attention)

è‡ªæ³¨æ„åŠ›æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å—, å®ƒä»ç„¶æ˜¯ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶, ä¸ç¼–ç -è§£ç å™¨ç»“æ„ä¸åŒçš„æ˜¯, è‡ªæ³¨æ„åŠ›æ¨¡å—åªä½¿ç”¨è¾“å…¥çš„ä¿¡æ¯è®¡ç®—åŠ æƒç³»æ•°, è€Œä¸éœ€è¦ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„ä¿¡æ¯, å› æ­¤å¯ä»¥å®ç°æ›´å¤§è§„æ¨¡çš„å¹¶è¡Œè®¡ç®—. Google åœ¨ 2017 å¹´æå‡ºçš„ Transformer å®ç°äº†å¯å¹¶è¡Œçš„è‡ªæ³¨æ„åŠ›æ¨¡å—[^9]. å…¶ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º.

{% include image.html class="polaroid" url="2018/01/self-attention.png" title="è¯­è¨€ç¿»è¯‘ä»»åŠ¡ä¸­çš„è‡ªæ³¨æ„åŠ›" %}

è‡ªæ³¨æ„åŠ›æ¨¡å—å·¥ä½œæ–¹å¼å¦‚ä¸‹[^10]:

* è¾“å…¥çš„åºåˆ—é¦–å…ˆè½¬åŒ–ä¸ºç›¸åŒé•¿åº¦çš„ embedding
* åˆ›å»ºä¸‰ä¸ªçŸ©é˜µ: æŸ¥è¯¢çŸ©é˜µ (Query), é”®çŸ©é˜µ (Key), å€¼çŸ©é˜µ (Value)
* ä½¿ç”¨è¿™ä¸‰ä¸ªçŸ©é˜µæŠŠæ¯ä¸ªå•è¯çš„ embedding æ˜ å°„ä¸ºä¸‰ä¸ªç¨çŸ­çš„ç‰¹å¾å‘é‡, åˆ†åˆ«ä»£è¡¨äº†å½“å‰å•è¯çš„æŸ¥è¯¢å‘é‡, é”®å‘é‡å’Œå€¼å‘é‡.
* æ¯”å¦‚æˆ‘ä»¬è¦è®¡ç®—å•è¯ Thinking å…³äºå¥å­ä¸­å…¶ä»–å•è¯çš„æ³¨æ„åŠ›æ—¶, ä½¿ç”¨ Thinking çš„æŸ¥è¯¢å‘é‡ä¾æ¬¡ä¸å¥å­ä¸­æ‰€æœ‰å•è¯ (åŒ…æ‹¬ Thinking) çš„é”®å‘é‡åšç‚¹ç§¯æ¥è®¡ç®—ç›¸ä¼¼åº¦, å¹¶å¯¹ç›¸ä¼¼åº¦è¿›è¡Œå½’ä¸€åŒ–å¾—åˆ°åŠ æƒç³»æ•° (è¿™æ˜¯ attention çš„æ ¸å¿ƒéƒ¨åˆ†).
* åŠ æƒç³»æ•°ä¹˜åˆ°æ‰€æœ‰å•è¯çš„å€¼å‘é‡ä¸Šæ¥å¾—åˆ°å•è¯ Thinking ç»è¿‡è‡ªæ³¨æ„åŠ›æ¨¡å—åè¾“å‡ºçš„ç‰¹å¾å‘é‡, è¿™ä¸ªç‰¹å¾å‘é‡ä¸­å¯ä»¥çœ‹ä½œåŒ…å«äº†ç¿»è¯‘ä»»åŠ¡é‡å¯¹å‡†ç¡®ç¿»è¯‘ Thinking æ‰€éœ€è¦çš„ä¿¡æ¯, è€Œä¸åŒ…å«å…¶ä»–ä¿¡æ¯ (å…¶ä»–ä¿¡æ¯åœ¨ç¿»è¯‘å…¶ä»–è¯æ—¶å¯èƒ½æœ‰ç”¨, ä½†åœ¨ç¿»è¯‘ Thinking æ—¶æ— ç”¨).


## 6. tensorflow å®˜æ–¹ä»£ç è§£æ

æ¥æº: [Tensorflow å¤šå±‚ LSTM ä»£ç åˆ†æ](http://blog.csdn.net/u014595019/article/details/52759104)

ä¿®æ”¹ä¸º tensorflow r1.4 ç‰ˆæœ¬

### åˆ†æ®µè®²è§£

æ€»çš„æ¥çœ‹ï¼Œè¿™ä»½ä»£ç ä¸»è¦ç”±ä¸‰æ­¥åˆ†ç»„æˆã€‚ 
ç¬¬ä¸€éƒ¨åˆ†ï¼Œæ˜¯PTBModel,ä¹Ÿæ˜¯æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼Œè´Ÿè´£tfä¸­æ¨¡å‹çš„æ„å»ºå’Œå„ç§æ“ä½œ(op)çš„å®šä¹‰ã€‚ 
ç¬¬äºŒéƒ¨åˆ†ï¼Œæ˜¯run_epochå‡½æ•°ï¼Œè´Ÿè´£å°†æ‰€æœ‰æ–‡æœ¬å†…å®¹åˆ†æ‰¹å–‚ç»™æ¨¡å‹ï¼ˆPTBModelï¼‰è®­ç»ƒã€‚ 
ç¬¬ä¸‰éƒ¨åˆ†ï¼Œå°±æ˜¯mainå‡½æ•°äº†ï¼Œè´Ÿè´£å°†ç¬¬äºŒéƒ¨åˆ†çš„run_epochè¿è¡Œå¤šéï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ–‡æœ¬ä¸­çš„æ¯ä¸ªå†…å®¹éƒ½ä¼šè¢«é‡å¤å¤šæ¬¡çš„è¾“å…¥åˆ°æ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œä¼šé€‚å½“çš„è¿›è¡Œä¸€äº›å‚æ•°çš„è°ƒæ•´ã€‚ 
ä¸‹é¢å°±æŒ‰ç…§è¿™å‡ éƒ¨åˆ†æ¥åˆ†å¼€è®²ä¸€ä¸‹ã€‚æˆ‘åœ¨åé¢æä¾›äº†å®Œæ•´çš„ä»£ç ï¼Œæ‰€ä»¥å¯ä»¥å°†å®Œæ•´ä»£ç å’Œåˆ†æ®µè®²è§£å¯¹ç…§ç€çœ‹ã€‚

---

### å‚æ•°è®¾ç½®

åœ¨æ„å»ºæ¨¡å‹å’Œè®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è®¾ç½®ä¸€äº›å‚æ•°ã€‚tfä¸­å¯ä»¥ä½¿ç”¨ `tf.flags` æ¥è¿›è¡Œå…¨å±€çš„å‚æ•°è®¾ç½®

```python
flags = tf.flags
logging = tf.logging    

flags.DEFINE_string(    # å®šä¹‰å˜é‡ modelçš„å€¼ä¸ºsmall, åé¢çš„æ˜¯æ³¨é‡Š
    "model", "small",
    "A type of model. Possible options are: small, medium, large.")

flags.DEFINE_string("data_path",   #å®šä¹‰ä¸‹è½½å¥½çš„æ•°æ®çš„å­˜æ”¾ä½ç½®
                    '/home/multiangle/download/simple-examples/data/', 
                    "data_path")
flags.DEFINE_bool("use_fp16", False,    # æ˜¯å¦ä½¿ç”¨ float16æ ¼å¼ï¼Ÿ
                  "Train using 16-bit floats instead of 32bit floats")

FLAGS = flags.FLAGS     # å¯ä»¥ä½¿ç”¨FLAGS.modelæ¥è°ƒç”¨å˜é‡ modelçš„å€¼ã€‚

def data_type():
    return tf.float16 if FLAGS.use_fp16 else tf.float321234567891011121314151617
```

ç»†å¿ƒçš„äººå¯èƒ½ä¼šæ³¨æ„åˆ°ä¸Šé¢æœ‰è¡Œä»£ç å®šä¹‰äº† model çš„å€¼ä¸º small.è¿™ä¸ªæ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿå…¶å®åœ¨åé¢çš„å®Œæ•´ä»£ç éƒ¨åˆ†å¯ä»¥çœ‹åˆ°ï¼Œä½œè€…åœ¨å…¶ä¸­å®šä¹‰äº†å‡ ä¸ªå‚æ•°ç±»ï¼Œåˆ†åˆ«æœ‰ small,medium,large å’Œ test è¿™ 4 ç§å‚æ•°ã€‚å¦‚æœ model çš„å€¼ä¸ºsmallï¼Œåˆ™ä¼šè°ƒç”¨ SmallConfigï¼Œå…¶ä»–åŒæ ·ã€‚åœ¨ SmallConfig ä¸­ï¼Œæœ‰å¦‚ä¸‹å‡ ä¸ªå‚æ•°ï¼š

```python
init_scale = 0.1        # ç›¸å…³å‚æ•°çš„åˆå§‹å€¼ä¸ºéšæœºå‡åŒ€åˆ†å¸ƒï¼ŒèŒƒå›´æ˜¯[-init_scale,+init_scale]
learning_rate = 1.0     # å­¦ä¹ é€Ÿç‡,åœ¨æ–‡æœ¬å¾ªç¯æ¬¡æ•°è¶…è¿‡max_epochä»¥åä¼šé€æ¸é™ä½
max_grad_norm = 5       # ç”¨äºæ§åˆ¶æ¢¯åº¦è†¨èƒ€ï¼Œå¦‚æœæ¢¯åº¦å‘é‡çš„L2æ¨¡è¶…è¿‡max_grad_normï¼Œåˆ™ç­‰æ¯”ä¾‹ç¼©å°
num_layers = 2          # lstmå±‚æ•°
num_steps = 20          # å•ä¸ªæ•°æ®ä¸­ï¼Œåºåˆ—çš„é•¿åº¦ã€‚
hidden_size = 200       # éšè—å±‚ä¸­å•å…ƒæ•°ç›®
max_epoch = 4           # epoch<max_epochæ—¶ï¼Œlr_decayå€¼=1,epoch>max_epochæ—¶,lr_decayé€æ¸å‡å°
max_max_epoch = 13      # æŒ‡çš„æ˜¯æ•´ä¸ªæ–‡æœ¬å¾ªç¯æ¬¡æ•°ã€‚
keep_prob = 1.0         # ç”¨äºdropout.æ¯æ‰¹æ•°æ®è¾“å…¥æ—¶ç¥ç»ç½‘ç»œä¸­çš„æ¯ä¸ªå•å…ƒä¼šä»¥1-keep_probçš„æ¦‚ç‡ä¸å·¥ä½œï¼Œå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
lr_decay = 0.5          # å­¦ä¹ é€Ÿç‡è¡°å‡
batch_size = 20         # æ¯æ‰¹æ•°æ®çš„è§„æ¨¡ï¼Œæ¯æ‰¹æœ‰20ä¸ªã€‚
vocab_size = 10000      # è¯å…¸è§„æ¨¡ï¼Œæ€»å…±10Kä¸ªè¯123456789101112
```

å…¶ä»–çš„å‡ ä¸ªå‚æ•°ç±»ä¸­ï¼Œå‚æ•°ç±»å‹éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯å‚æ•°çš„å€¼å„æœ‰æ‰€ä¸åŒã€‚

---

### PTBModel

è¿™ä¸ªå¯ä»¥è¯´æ˜¯æ ¸å¿ƒéƒ¨åˆ†äº†ã€‚è€Œå…·ä½“æ¥è¯´ï¼Œåˆå¯ä»¥åˆ†æˆå‡ ä¸ªå°éƒ¨åˆ†ï¼š

* å¤šå±‚LSTMç»“æ„çš„æ„å»º
* è¾“å…¥é¢„å¤„ç†
* LSTMçš„å¾ªç¯
* æŸå¤±å‡½æ•°è®¡ç®—
* æ¢¯åº¦è®¡ç®—
* ä¿®å‰ª

#### 1.1 LSTMç»“æ„

```python
class PTBInput(object):
  """The input data."""

  def __init__(self, config, data, name=None):
    self.batch_size = batch_size = config.batch_size
    self.num_steps = num_steps = config.num_steps
    self.epoch_size = ((len(data) // batch_size) - 1) // num_steps
    self.input_data, self.targets = reader.ptb_producer(
        data, batch_size, num_steps, name=name)		# è¯¥ç±»ä¸»è¦ä½œç”¨æ˜¯è¿™å¥, ä»æ–‡ä»¶ä¸­è¯»å–æ•°æ®
```

**self.input_data å’Œ self.targets éƒ½æ˜¯ index çš„åºåˆ—, å°ºå¯¸ä¸º [batch_size, num_steps]**. æ³¨æ„æ­¤æ—¶ä¸è®ºæ˜¯inputè¿˜æ˜¯targetéƒ½æ˜¯ç”¨è¯å…¸idæ¥è¡¨ç¤ºå•è¯çš„ã€‚

`PTBModel.__init__()` å‡½æ•°:

```python
self._input = input_	# [batch_size, num_steps]

batch_size = input_.batch_size
num_steps = input_.num_steps
size = config.hidden_size		# éšè—å±‚è§„æ¨¡
vocab_size = config.vocab_size	# è¯å…¸è§„æ¨¡
```

å¼•è¿›å‚æ•°.

```python
def lstm_cell():
      if 'reuse' in inspect.getargspec(tf.contrib.rnn.BasicLSTMCell.__init__).args:
        return tf.contrib.rnn.BasicLSTMCell(size, 
                                            forget_bias=0.0, 
                                            state_is_tuple=True,
                                            reuse=tf.get_variable_scope().reuse)
      else:
        return tf.contrib.rnn.BasicLSTMCell(size, 
                                            forget_bias=0.0, 
                                            state_is_tuple=True)
```

é¦–å…ˆä½¿ç”¨ `tf.contrib.rnn.BasicLSTMCell` å®šä¹‰å•ä¸ªåŸºæœ¬çš„ LSTM å•å…ƒã€‚è¿™é‡Œçš„ size å…¶å®å°±æ˜¯éšè—å±‚è§„æ¨¡ã€‚ 
ä»æºç ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ LSTM å•å…ƒä¸­ï¼Œæœ‰ 2 ä¸ªçŠ¶æ€å€¼ï¼Œåˆ†åˆ«æ˜¯ c å’Œ hï¼Œåˆ†åˆ«å¯¹åº”äºä¸‹å›¾ä¸­çš„ c å’Œ hã€‚å…¶ä¸­ h åœ¨ä½œä¸ºå½“å‰æ—¶é—´æ®µçš„è¾“å‡ºçš„åŒæ—¶ï¼Œä¹Ÿæ˜¯ä¸‹ä¸€æ—¶é—´æ®µçš„è¾“å…¥çš„ä¸€éƒ¨åˆ†ã€‚

{% include image.html class="polaroid-small" url="/2018-1-24/lstm-1.png" title="LSTM å•å…ƒ" %}

é‚£ä¹ˆå½“ `state_is_tuple=True` çš„æ—¶å€™ï¼Œstate æ˜¯å…ƒç»„å½¢å¼ï¼Œstate=(c,h)ã€‚å¦‚æœæ˜¯ Falseï¼Œé‚£ä¹ˆ state æ˜¯ä¸€ä¸ªç”±cå’Œhæ‹¼æ¥èµ·æ¥çš„å¼ é‡ï¼Œ`state=tf.concat(1, [c,h])`ã€‚**åœ¨è¿è¡Œæ—¶ï¼Œåˆ™è¿”å›2å€¼ï¼Œä¸€ä¸ªæ˜¯hï¼Œè¿˜æœ‰ä¸€ä¸ªstateã€‚**

#### 1.2 DropoutWrapper

```python
attn_cell = lstm_cell
if is_training and config.keep_prob < 1:	# åœ¨å¤–é¢åŒ…è£¹ dropout
	def attn_cell():
        return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=config.keep_prob)
```

æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨äº† dropout æ–¹æ³•ã€‚**æ‰€è°“ dropout, å°±æ˜¯æŒ‡ç½‘ç»œä¸­æ¯ä¸ªå•å…ƒåœ¨æ¯æ¬¡æœ‰æ•°æ®æµå…¥æ—¶ä»¥ä¸€å®šçš„æ¦‚ç‡(keep prob)æ­£å¸¸å·¥ä½œï¼Œå¦åˆ™è¾“å‡º0å€¼**ã€‚è¿™æ˜¯æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆã€‚*åœ¨ rnn ä¸­ä½¿ç”¨ dropout çš„æ–¹æ³•å’Œ cnn ä¸åŒ*ï¼Œæ¨èå¤§å®¶å»æŠŠ [recurrent neural network regularization](http://arxiv.org/pdf/1409.2329.pdf) çœ‹ä¸€éã€‚ 
åœ¨ rnn ä¸­è¿›è¡Œ dropout æ—¶ï¼Œå¯¹äº rnn çš„éƒ¨åˆ†ä¸è¿›è¡Œ dropoutï¼Œä¹Ÿå°±æ˜¯è¯´ä» t-1 æ—¶å€™çš„çŠ¶æ€ä¼ é€’åˆ°tæ—¶åˆ»è¿›è¡Œè®¡ç®—æ—¶ï¼Œè¿™ä¸ªä¸­é—´ä¸è¿›è¡Œ memory çš„ dropoutï¼›ä»…åœ¨åŒä¸€ä¸ªtæ—¶åˆ»ä¸­ï¼Œå¤šå±‚ cell ä¹‹é—´ä¼ é€’ä¿¡æ¯çš„æ—¶å€™è¿›è¡Œ dropoutï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º

{% include image.html class="polaroid-small" url="/2018-1-24/dropout.jpg" title="å¾ªç¯ç¥ç»å…ƒå±•å¼€ç¤ºæ„å›¾" %}

ä¸Šå›¾ä¸­ï¼Œ$$ t-2 $$ æ—¶åˆ»çš„è¾“å…¥ $$ x_{tâˆ’2} $$ é¦–å…ˆä¼ å…¥ç¬¬ä¸€å±‚ cellï¼Œè¿™ä¸ªè¿‡ç¨‹æœ‰ dropoutï¼Œä½†æ˜¯ä»  $$ tâˆ’2 $$ æ—¶åˆ»çš„ç¬¬ä¸€å±‚ cell ä¼ åˆ° $$ tâˆ’1, t, t+1 $$ çš„ç¬¬ä¸€å±‚ cell è¿™ä¸ªä¸­é—´éƒ½ä¸è¿›è¡Œ dropoutã€‚å†ä» $$ t+1 $$ æ—¶å€™çš„ç¬¬ä¸€å±‚ cell å‘åŒä¸€æ—¶åˆ»å†…åç»­çš„ cell ä¼ é€’æ—¶ï¼Œè¿™ä¹‹é—´åˆæœ‰ dropout äº†ã€‚

åœ¨ä½¿ç”¨ `tf.contrib.rnn.DropoutWrapper` æ—¶ï¼ŒåŒæ ·æœ‰ä¸€äº›å‚æ•°ï¼Œä¾‹å¦‚ `input_keep_prob, output_keep_prob` ç­‰ï¼Œåˆ†åˆ«æ§åˆ¶è¾“å…¥å’Œè¾“å‡ºçš„dropoutæ¦‚ç‡ï¼Œå¾ˆå¥½ç†è§£ã€‚

#### 1.3 å¤šå±‚LSTMç»“æ„å’ŒçŠ¶æ€åˆå§‹åŒ–

```python
cell = tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(config.num_layers)],
                                   state_is_tuple=True)
# å‚æ•°åˆå§‹åŒ–,rnn_cell.RNNCell.zero_stat
self._initial_state = cell.zero_state(batch_size, data_type())
```

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† 2 å±‚çš„ LSTM ç½‘ç»œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå‰ä¸€å±‚çš„ LSTM çš„è¾“å‡ºä½œä¸ºåä¸€å±‚çš„è¾“å…¥ã€‚ä½¿ç”¨`tf.contrib.rnn.MultiRNNCell` å¯ä»¥å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚è¿™ä¸ªåŸºæœ¬æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œ`state_is_tuple` ç”¨æ³•ä¹Ÿè·Ÿä¹‹å‰çš„ç±»ä¼¼ã€‚æ„é€ å®Œå¤šå±‚ LSTM ä»¥åï¼Œä½¿ç”¨ `zero_state` å³å¯å¯¹å„ç§çŠ¶æ€è¿›è¡Œåˆå§‹åŒ–ã€‚

#### 2. è¾“å…¥é¢„å¤„ç†

```python
with tf.device("/cpu:0"):
  	# æŠŠæè¿°å•è¯çš„æŒ‡æ ‡ idx ([1, 1]) å˜ä¸º embedding ([1, hidden_size]) æè¿°
    # ä½¿ç”¨ embedding æè¿°å¯ä»¥è®©ç½‘ç»œä»æè¿°ä¸­å­¦ä¹ å•è¯ä¹‹é—´çš„å…³è”, å¦åˆ™å•ä¸ªçš„æŒ‡æ ‡ä¹‹é—´æ˜¯ç‹¬ç«‹çš„, æ— æ³•å­¦ä¹ å…³è” ???
    embedding = tf.get_variable("embedding", [vocab_size, size], dtype=data_type())     

    # å°†è¾“å…¥çš„æ¯ä¸ª sequence ([batch_size, num_steps]) ç”¨ embedding è¡¨ç¤º 
    # shape = [batch_size, num_steps, hidden_size]
    # æ‰€ä»¥æ¯ä¸ª x_t éƒ½æ˜¯ä¸€ä¸ª batch_size x 1 x hidden_size çš„å‘é‡
    # åœ¨ç¨‹åºé‡Œ 1 è‡ªåŠ¨çœå», æ‰€ä»¥æ¯ä¸ª x_t å®é™…ä¸Šæ˜¯ [batch_size, hidden_size] çš„å‘é‡
    inputs = tf.nn.embedding_lookup(embedding, input_.input_data)

if is_training and config.keep_prob < 1:
    inputs = tf.nn.dropout(inputs, config.keep_prob)
```

ä¹‹å‰æœ‰æåˆ°è¿‡ï¼Œè¾“å…¥æ¨¡å‹çš„ input å’Œ target éƒ½æ˜¯ç”¨è¯å…¸ id è¡¨ç¤ºçš„ã€‚ä¾‹å¦‚ä¸€ä¸ªå¥å­ï¼Œâ€œæˆ‘/æ˜¯/å­¦ç”Ÿâ€ï¼Œè¿™ä¸‰ä¸ªè¯åœ¨è¯å…¸ä¸­çš„åºå·åˆ†åˆ«æ˜¯ 0,5,3ï¼Œé‚£ä¹ˆä¸Šé¢çš„å¥å­å°±æ˜¯ [0,5,3]ã€‚è¿™æ ·å°±å’Œéšè—å±‚éœ€è¦çš„è¾“å…¥ç»´åº¦ä¸åŒ¹é… (è¾“å…¥éœ€è¦é•¿åº¦ä¸º hidden_size çš„å‘é‡)ï¼Œæˆ‘ä»¬è¦æŠŠè¯å…¸ id è½¬åŒ–æˆå‘é‡,ä¹Ÿå°±æ˜¯ embedding å½¢å¼ã€‚å¯èƒ½æœ‰äº›äººå·²ç»å¬åˆ°è¿‡è¿™ç§æè¿°äº†ã€‚å®ç°çš„æ–¹æ³•å¾ˆç®€å•ã€‚

ç¬¬ä¸€æ­¥ï¼Œæ„å»ºä¸€ä¸ªçŸ©é˜µï¼Œå°±å« embedding å¥½äº†ï¼Œå°ºå¯¸ä¸º [vocab_size, embedding_size]ï¼Œåˆ†åˆ«è¡¨ç¤ºè¯å…¸ä¸­å•è¯æ•°ç›®ï¼Œä»¥åŠè¦è½¬åŒ–æˆçš„å‘é‡çš„ç»´åº¦ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå‘é‡ç»´åº¦è¶Šé«˜ï¼Œèƒ½å¤Ÿè¡¨ç°çš„ä¿¡æ¯ä¹Ÿå°±è¶Šä¸°å¯Œã€‚æ³¨æ„è¿™é‡Œçš„ embedding å˜é‡æ˜¯ä½¿ç”¨å‡åŒ€åˆ†å¸ƒéšæœºåˆå§‹åŒ–çš„, åˆå§‹åŒ–å™¨å®šä¹‰åœ¨ main å‡½æ•°ä¸­. å¹¶ä¸” embedding æ˜¯å˜é‡çŸ©é˜µ, åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯è¿›è¡Œä¼˜åŒ–çš„, æœŸæœ›æ˜¯è®©æœ‰å…³è”çš„è¯è¯­å¯¹åº”çš„ embedding å‘é‡å½¢æˆæŸç§å…³ç³».

ç¬¬äºŒæ­¥ï¼Œä½¿ç”¨ `tf.nn.embedding_lookup(embedding,input_ids)` å‡è®¾ input_ids çš„é•¿åº¦ä¸º lenï¼Œé‚£ä¹ˆè¿”å›çš„å¼ é‡å°ºå¯¸å°±ä¸º [len,embedding_size]ã€‚

ä¸¾ä¸ªæ —å­:

```python
# ç¤ºä¾‹ä»£ç 
import tensorflow as tf
import numpy as np

sess = tf.InteractiveSession()

embedding = tf.Variable(np.identity(5,dtype=np.int32))
input_ids = tf.placeholder(dtype=tf.int32,shape=[None])
input_embedding = tf.nn.embedding_lookup(embedding,input_ids)

sess.run(tf.initialize_all_variables())
print(sess.run(embedding))
#[[1 0 0 0 0]
# [0 1 0 0 0]
# [0 0 1 0 0]
# [0 0 0 1 0]
# [0 0 0 0 1]]
print(sess.run(input_embedding,feed_dict={input_ids:[1,2,3,0,3,2,1]}))
#[[0 1 0 0 0]
# [0 0 1 0 0]
# [0 0 0 1 0]
# [1 0 0 0 0]
# [0 0 0 1 0]
# [0 0 1 0 0]
# [0 1 0 0 0]]
```

ç¬¬ä¸‰æ­¥ï¼Œå¦‚æœ keep_prob<1ï¼Œ é‚£ä¹ˆè¿˜éœ€è¦å¯¹è¾“å…¥è¿›è¡Œ dropoutã€‚ä¸è¿‡è¿™è¾¹è·Ÿ rnn çš„ dropout åˆæœ‰æ‰€ä¸åŒï¼Œè¿™è¾¹ä½¿ç”¨ `tf.nn.dropout`ã€‚

#### 3. LSTMå¾ªç¯

ç°åœ¨ï¼Œå¤šå±‚ lstm å•å…ƒå·²ç»å®šä¹‰å®Œæ¯•ï¼Œè¾“å…¥ä¹Ÿå·²ç»ç»è¿‡é¢„å¤„ç†äº†ã€‚é‚£ä¹ˆç°åœ¨è¦åšçš„å°±æ˜¯å°†æ•°æ®è¾“å…¥lstmè¿›è¡Œè®­ç»ƒäº†ã€‚å…¶å®å¾ˆç®€å•ï¼Œåªè¦æŒ‰ç…§æ–‡æœ¬é¡ºåºä¾æ¬¡å‘cellè¾“å…¥æ•°æ®å°±å¥½äº†ã€‚lstmä¸Šä¸€æ—¶é—´æ®µçš„çŠ¶æ€ä¼šè‡ªåŠ¨å‚ä¸åˆ°å½“å‰æ—¶é—´æ®µçš„è¾“å‡ºå’ŒçŠ¶æ€çš„è®¡ç®—å½“ä¸­ã€‚

```python
outputs = []
state = self._initial_state # state è¡¨ç¤º å„ä¸ªbatchä¸­çš„çŠ¶æ€
with tf.variable_scope("RNN"):
    for time_step in range(num_steps):
        if time_step > 0: tf.get_variable_scope().reuse_variables()
        # è¾“å…¥: [batch_size, hidden_size]
        # æŒ‰ç…§é¡ºåºå‘cellè¾“å…¥æ–‡æœ¬æ•°æ®, cell_out: [batch_size, hidden_size]
        (cell_output, state) = cell(inputs[:, time_step, :], state) 
        outputs.append(cell_output)  # output: [num_steps][batch_size, hidden_size]

# æŠŠä¹‹å‰çš„listå±•å¼€ï¼Œæˆ [batch, num_steps, hidden_size]
# ç„¶å reshape æˆ [batch*numsteps, hidden_size], è¿™æ˜¯ä¸ºäº†åé¢ softmax å±‚è®¡ç®—æ–¹ä¾¿
output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])
```

è¿™è¾¹è¦æ³¨æ„ï¼Œ`tf.get_variable_scope().reuse_variables()` è¿™è¡Œä»£ç ä¸å¯å°‘ï¼Œå› ä¸ºåœ¨ num_steps çš„å¾ªç¯ä¸­å®é™…ä¸Šæ˜¯åœ¨ç›¸åŒçš„æƒé‡ä¸Šæ›´æ–°çš„

#### 4. æŸå¤±å‡½æ•°è®¡ç®—

```python
# softmax_w , shape=[hidden_size, vocab_size], ç”¨äºå°†distributedè¡¨ç¤ºçš„å•è¯è½¬åŒ–ä¸ºone-hotè¡¨ç¤º
softmax_w = tf.get_variable("softmax_w", [size, vocab_size], dtype=data_type())
softmax_b = tf.get_variable("softmax_b", [vocab_size], dtype=data_type())
# [batch*numsteps, vocab_size] ä»éšè—è¯­ä¹‰è½¬åŒ–æˆå®Œå…¨è¡¨ç¤º
logits = tf.matmul(output, softmax_w) + softmax_b

# loss , shape=[batch*num_steps], å¸¦æƒé‡çš„äº¤å‰ç†µè®¡ç®—
loss = tf.contrib.seq2seq.sequence_loss(
    logits,					# [batch*numsteps, vocab_size]
    input_.targets,			# [batch_size, num_steps]
    tf.ones([batch_size, num_steps], dtype=data_type()),	# weight
    average_across_timesteps=False,
    average_across_batch=True			# loss = loss / batch_size
)
self._cost = cost = tf.reduce_sum(loss)
self._final_state = state
```

ä¸Šé¢ä»£ç çš„ä¸ŠåŠéƒ¨åˆ†ä¸»è¦ç”¨æ¥å°†å¤šå±‚lstmå•å…ƒçš„è¾“å‡ºè½¬åŒ–æˆone-hotè¡¨ç¤ºçš„å‘é‡ã€‚å…³äºone-hot presentationå’Œdistributed presentationçš„åŒºåˆ«ï¼Œå¯ä»¥å‚è€ƒ [è¿™é‡Œ](http://blog.csdn.net/u014595019/article/details/51884529#t0)

ä»£ç çš„ä¸‹åŠéƒ¨åˆ†ï¼Œæ­£å¼å¼€å§‹è®¡ç®—æŸå¤±å‡½æ•°ã€‚è¿™é‡Œä½¿ç”¨äº† tf æä¾›çš„ç°æˆçš„äº¤å‰ç†µè®¡ç®—å‡½æ•°, `tf.contrib.seq2seq.sequence_loss`ã€‚ä¸çŸ¥é“äº¤å‰ç†µæ˜¯ä»€ä¹ˆï¼Ÿè§[è¿™é‡Œ](http://blog.csdn.net/u014595019/article/details/52562159#t7). å„ä¸ªå˜é‡çš„å…·ä½“shapeæˆ‘éƒ½åœ¨æ³¨é‡Šä¸­æ ‡æ˜äº†ã€‚æ³¨æ„å…¶ä¸­çš„ `self._targets` æ˜¯è¯å…¸ id è¡¨ç¤ºçš„ã€‚è¿™ä¸ªå‡½æ•°çš„å…·ä½“å®ç°æ–¹å¼ä¸æ˜ã€‚æˆ‘æ›¾ç»æƒ³è‡ªå·±æ‰‹å†™ä¸€ä¸ªäº¤å‰ç†µï¼Œä¸è¿‡å¥½åƒ tf ä¸æ”¯æŒå¯¹å¼ é‡ä¸­å•ä¸ªå…ƒç´ çš„æ“ä½œã€‚

#### 5. æ¢¯åº¦è®¡ç®—

ä¹‹å‰å·²ç»è®¡ç®—å¾—åˆ°äº†æ¯æ‰¹æ•°æ®çš„å¹³å‡è¯¯å·®ã€‚é‚£ä¹ˆä¸‹ä¸€æ­¥ï¼Œå°±æ˜¯æ ¹æ®è¯¯å·®æ¥è¿›è¡Œå‚æ•°ä¿®æ­£äº†ã€‚å½“ç„¶ï¼Œé¦–å…ˆå¿…é¡»è¦æ±‚æ¢¯åº¦

```python
self._lr = tf.Variable(0.0, trainable=False)  # lr æŒ‡çš„æ˜¯ learning_rate
tvars = tf.trainable_variables()
```

**é€šè¿‡ tf.trainable_variables å¯ä»¥å¾—åˆ°æ•´ä¸ªæ¨¡å‹ä¸­æ‰€æœ‰ trainable = True çš„ Variable**ã€‚å®é™…å¾—åˆ°çš„ tvars æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé‡Œé¢å­˜æœ‰æ‰€æœ‰å¯ä»¥è¿›è¡Œè®­ç»ƒçš„å˜é‡ã€‚

```python
grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                  config.max_grad_norm)
```

è¿™ä¸€è¡Œä»£ç å…¶å®ä½¿ç”¨äº†ä¸¤ä¸ªå‡½æ•°ï¼Œ`tf.gradients` å’Œ `tf.clip_by_global_norm`ã€‚ æˆ‘ä»¬ä¸€ä¸ªä¸€ä¸ªæ¥ã€‚

**tf.gradients** 
ç”¨æ¥è®¡ç®—å¯¼æ•°ã€‚è¯¥å‡½æ•°çš„å®šä¹‰å¦‚ä¸‹æ‰€ç¤º

```python
def gradients(ys,
              xs,
              grad_ys=None,
              name="gradients",
              colocate_gradients_with_ops=False,
              gate_gradients=False,
              aggregation_method=None):
```

è™½ç„¶å¯é€‰å‚æ•°å¾ˆå¤šï¼Œä½†æ˜¯æœ€å¸¸ä½¿ç”¨çš„è¿˜æ˜¯yså’Œxsã€‚æ ¹æ®è¯´æ˜å¾—çŸ¥ï¼Œyså’Œxséƒ½å¯ä»¥æ˜¯ä¸€ä¸ªtensoræˆ–è€…tensoråˆ—è¡¨ã€‚è€Œè®¡ç®—å®Œæˆä»¥åï¼Œè¯¥å‡½æ•°ä¼šè¿”å›ä¸€ä¸ªé•¿ä¸ºlen(xs)çš„tensoråˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªtensoræ˜¯ysä¸­æ¯ä¸ªå€¼å¯¹xs[i]æ±‚å¯¼ä¹‹å’Œã€‚å¦‚æœç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºçš„è¯ï¼Œé‚£ä¹ˆ `g = tf.gradients(y,x)`å¯ä»¥è¡¨ç¤ºæˆ 

$$
g_i = \sum_{j=0}^{len(y)}\frac{\partial y_i}{\partial x_i} \\ g = [g_0, g_1, \cdots, g_{len(x)}
$$

#### 6. æ¢¯åº¦ä¿®å‰ª

**tf.clip_by_global_norm** 
ä¿®æ­£æ¢¯åº¦å€¼ï¼Œç”¨äº**æ§åˆ¶æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜**ã€‚æ¢¯åº¦çˆ†ç‚¸å’Œæ¢¯åº¦å¼¥æ•£çš„åŸå› ä¸€æ ·ï¼Œéƒ½æ˜¯å› ä¸ºé“¾å¼æ³•åˆ™æ±‚å¯¼çš„å…³ç³»ï¼Œå¯¼è‡´æ¢¯åº¦çš„æŒ‡æ•°çº§è¡°å‡ã€‚ä¸ºäº†é¿å…æ¢¯åº¦çˆ†ç‚¸ï¼Œéœ€è¦å¯¹æ¢¯åº¦è¿›è¡Œä¿®å‰ªã€‚ 
å…ˆæ¥çœ‹è¿™ä¸ªå‡½æ•°çš„å®šä¹‰ï¼š

```python
def clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None):
```

**è¾“å…¥å‚æ•°**ä¸­ï¼št_list ä¸ºå¾…ä¿®å‰ªçš„å¼ é‡, clip_norm è¡¨ç¤ºä¿®å‰ªæ¯”ä¾‹ (clipping ratio).

å‡½æ•°**è¿”å›2ä¸ªå‚æ•°**ï¼š list_clippedï¼Œä¿®å‰ªåçš„å¼ é‡ï¼Œä»¥åŠglobal_normï¼Œä¸€ä¸ªä¸­é—´è®¡ç®—é‡ã€‚å½“ç„¶å¦‚æœä½ ä¹‹å‰å·²ç»è®¡ç®—å‡ºäº† global_norm å€¼ï¼Œä½ å¯ä»¥åœ¨ use_norm é€‰é¡¹ç›´æ¥æŒ‡å®š global_norm çš„å€¼ã€‚

é‚£ä¹ˆå…·ä½“**å¦‚ä½•è®¡ç®—**å‘¢ï¼Ÿæ ¹æ®æºç ä¸­çš„è¯´æ˜ï¼Œå¯ä»¥å¾—åˆ° 
`list_clipped[i] = t_list[i] * clip_norm / max(global_norm, clip_norm)`,å…¶ä¸­ 
`global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))`

å¦‚æœä½ æ›´ç†Ÿæ‚‰æ•°å­¦å…¬å¼ï¼Œåˆ™å¯ä»¥å†™ä½œ 

$$
L_c^i = \frac{L_t^i N_c}{\max(N_c, N_g)},\quad N_g = \sqrt{\sum_i(L_t^i)^2}
$$
å…¶ä¸­, $$ L_t^i $$ å’Œ $$ L_c^i $$ ä»£è¡¨ `t_list[i]` å’Œ `list_clipped[i]`, $$ N_c $$ å’Œ $$ N_g $$ ä»£è¡¨ clip_norm å’Œ global_norm çš„å€¼ã€‚ 

#### 7. ä¼˜åŒ–å‚æ•°

ä¹‹å‰çš„ä»£ç å·²ç»æ±‚å¾—äº†åˆé€‚çš„æ¢¯åº¦ï¼Œç°åœ¨éœ€è¦ä½¿ç”¨è¿™äº›æ¢¯åº¦æ¥æ›´æ–°å‚æ•°çš„å€¼äº†ã€‚

```python
# æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼ŒæŒ‡å®šå­¦ä¹ é€Ÿç‡
optimizer = tf.train.GradientDescentOptimizer(self._lr)
# optimizer = tf.train.AdamOptimizer()
# optimizer = tf.train.GradientDescentOptimizer(0.5)
self._train_op = optimizer.apply_gradients(
    zip(grads, tvars),
    global_step=tf.contrib.framework.get_or_create_global_step())  # å°†æ¢¯åº¦åº”ç”¨äºå˜é‡
```

è¿™ä¸€éƒ¨åˆ†å°±æ¯”è¾ƒè‡ªç”±äº†ï¼Œtf æä¾›äº†å¾ˆå¤šç§ä¼˜åŒ–å™¨ï¼Œä¾‹å¦‚æœ€å¸¸ç”¨çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼ˆGradientDescentOptimizerï¼‰ä¹Ÿå¯ä»¥ä½¿ç”¨ AdamOptimizerã€‚è¿™é‡Œä½¿ç”¨çš„æ˜¯æ¢¯åº¦ä¼˜åŒ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œä½¿ç”¨äº†optimizer.apply_gradientsæ¥å°†æ±‚å¾—çš„æ¢¯åº¦ç”¨äºå‚æ•°ä¿®æ­£ï¼Œè€Œä¸æ˜¯ä¹‹å‰ç®€å•çš„optimizer.minimize(cost)

è¿˜æœ‰ä¸€ç‚¹ï¼Œè¦ç•™å¿ƒä¸€ä¸‹ self._train_opï¼Œåªæœ‰è¯¥æ“ä½œè¢«æ¨¡å‹æ‰§è¡Œï¼Œæ‰èƒ½å¯¹å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚å¦‚æœæ²¡æœ‰æ‰§è¡Œè¯¥æ“ä½œï¼Œåˆ™å‚æ•°å°±ä¸ä¼šè¢«ä¼˜åŒ–ã€‚

### 8. Main å‡½æ•°

```python
initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)

with tf.name_scope("Train"):
	train_input = PTBInput(config=config, data=train_data, name="TrainInput")
	with tf.variable_scope("Model", reuse=None, initializer=initializer):
    	m = PTBModel(is_training=True, config=config, input_=train_input)
	tf.summary.scalar("Training Loss", m.cost)
	tf.summary.scalar("Learning Rate", m.lr)
```

ä»¥å‡åŒ€åˆ†å¸ƒ [-init_scale, init_scale] ä½œä¸ºæ‰€æœ‰å˜é‡çš„åˆå§‹åŒ–å™¨, `train_input` æ˜¯ä¸€ä¸ªç±», åŒ…å«äº†æ‰€æœ‰çš„è®­ç»ƒæ•°æ®.


## References

[^1]:
    **Understanding LSTM Networks -- Colah's blogs** <br />
    [[link]](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) OnLine.

[^2]:
    **è¯¦è§£ä» Seq2Seqæ¨¡å‹ã€RNNç»“æ„ã€Encoder-Decoderæ¨¡å‹ åˆ° Attentionæ¨¡å‹** <br />
    [[link]](https://caicai.science/2018/10/06/attention%E6%80%BB%E8%A7%88/) OnLine.

[^3]:
    **Learning phrase representations using RNN encoder-decoder for statistical machine translation** <br />
    Cho K, Van MerriÃ«nboer B, Gulcehre C, et al. <br />
    [[link]](https://arxiv.org/abs/1406.1078) In arXiv preprint arXiv:1406.1078, 2014.

[^4]:
    **Sequence to Sequence Learning with Neural Networks** <br />
    Ilya Sutskever, Oriol Vinyals, Quoc V.Le.  <br />
    [[link]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks) In Advances in neural information processing systems. 2014: 3104-3112.

[^5]:
    **Order Matters: Sequence to Sequence for Sets** <br />
    Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. <br />
    [[link]](http://arxiv.org/abs/1511.06391.) In ArXiv:1511.06391, November. 2015.

[^6]:
    **Long short-term memory** <br />
    Hochreiter S, Schmidhuber J. <br />
    [[link]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf) In Neural computation, 1997, 9(8): 1735-1780.

[^7]:
    **Recurrent nets that time and count** <br />
    Gers F A, Schmidhuber J. <br />
    [[link]](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf) In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. 2000.

[^8]:
    **Learning phrase representations using RNN encoder-decoder for statistical machine translation** <br />
    Cho K, Van MerriÃ«nboer B, Gulcehre C, et al. <br />
    [[link]](https://arxiv.org/pdf/1406.1078) In arXiv preprint arXiv:1406.1078, 2014.

[^9]:
    **Attention is all you need** <br />
    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin <br />
    [[link]](https://arxiv.org/abs/1706.03762) Advances in neural information processing systems. 2017: 5998-6008.

[^10]:
    **The Illustrated Transformer**  <br />
    Jay Alammar <br />
    [[link]](https://jalammar.github.io/illustrated-transformer/) OnLine.

[^11]:
    **Neural machine translation by jointly learning to align and translate** <br />
    Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio <br />
    [[link]](https://arxiv.org/abs/1409.0473) arXiv preprint arXiv:1409.0473, 2014. 
